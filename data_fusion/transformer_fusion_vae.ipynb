{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fd0e14-16e1-46b3-b436-a06f0bf1eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40adcac1-3511-4471-8168-e508fec26fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, query, key_value):\n",
    "        attn_output, _ = self.cross_attn(query, key_value, key_value)\n",
    "        return attn_output\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "class TransformerFusionModel(nn.Module):\n",
    "    def __init__(self, d_model=512, n_heads=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.audio_proj = nn.Linear(1024, d_model)\n",
    "        self.visual_proj = nn.Linear(512, d_model)\n",
    "        self.text_proj = nn.Linear(768, d_model)\n",
    "\n",
    "        self.va_cross = CrossAttention(d_model, n_heads)\n",
    "        self.vt_cross = CrossAttention(d_model, n_heads)\n",
    "        self.av_cross = CrossAttention(d_model, n_heads)\n",
    "        self.at_cross = CrossAttention(d_model, n_heads)\n",
    "        self.tv_cross = CrossAttention(d_model, n_heads)\n",
    "        self.ta_cross = CrossAttention(d_model, n_heads)\n",
    "\n",
    "        self.visual_self = SelfAttention(d_model*2, n_heads)\n",
    "        self.audio_self = SelfAttention(d_model*2, n_heads)\n",
    "        self.text_self = SelfAttention(d_model*2, n_heads)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(6*d_model, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, audio, vision, text):\n",
    "        audio = self.audio_proj(audio) \n",
    "        vision = self.visual_proj(vision)\n",
    "        text = self.text_proj(text)\n",
    "\n",
    "        v_a = self.va_cross(vision, audio)\n",
    "        v_t = self.vt_cross(vision, text)\n",
    "        a_v = self.av_cross(audio, vision)\n",
    "        a_t = self.at_cross(audio, text)\n",
    "        t_v = self.tv_cross(text, vision)\n",
    "        t_a = self.ta_cross(text, audio)\n",
    "\n",
    "        vision_cat = torch.cat([v_a, v_t], dim=-1)\n",
    "        audio_cat = torch.cat([a_v, a_t], dim=-1)\n",
    "        text_cat = torch.cat([t_v, t_a], dim=-1)\n",
    "\n",
    "        vision_final = self.visual_self(vision_cat)\n",
    "        audio_final = self.audio_self(audio_cat)\n",
    "        text_final = self.text_self(text_cat)\n",
    "\n",
    "\n",
    "        fused = torch.cat([vision_final, audio_final, text_final], dim=-1) \n",
    "        fused = fused.mean(dim=1) \n",
    "\n",
    "\n",
    "        out = self.fc(fused)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class PersonalityDatasetGenTest(Dataset):\n",
    "    def __init__(self, pkl_file):\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "        self.keys = list(self.data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "        sample = self.data[key]\n",
    "\n",
    "        audio = sample.get('original_audio', None)\n",
    "        if audio == None: \n",
    "            print(\"audio for\", idx, \"is None\")\n",
    "            new_idx = (idx + 1) % len(self.keys)\n",
    "            return self.__getitem__(new_idx)\n",
    "\n",
    "        \n",
    "        if audio.shape[-1]!=5: \n",
    "\n",
    "            new_idx = (idx + 1) % len(self.keys)\n",
    "            return self.__getitem__(new_idx)\n",
    "        if audio.ndim == 3:   # (5, 149, 1024)\n",
    "            audio = audio.mean(dim = 1) \n",
    "        audio = audio.float()\n",
    "        audio = audio.transpose(0, 1)\n",
    " \n",
    "        vision = sample.get('generated_vision', None)\n",
    "        if vision == None: \n",
    "            print(\"vision for\", idx, \"is None\")\n",
    "            new_idx = (idx + 1) % len(self.keys)\n",
    "            return self.__getitem__(new_idx)\n",
    "        # vision = sample['reconstructed_visual']\n",
    "        # print(vision.shape)\n",
    "        if vision.shape[0]!=5: \n",
    "            new_idx = (idx + 1) % len(self.keys)\n",
    "            return self.__getitem__(new_idx)\n",
    "        if vision.ndim == 3:  # (5, 5, 512) ?\n",
    "            vision = vision.squeeze(1)  # (5, 512)\n",
    "        vision = vision.float()\n",
    "\n",
    "        text = sample['original_text'].unsqueeze(0).repeat(5, 1).float()\n",
    "\n",
    "\n",
    "        labels = torch.tensor([\n",
    "            sample['openness'],\n",
    "            sample['conscientiousness'],\n",
    "            sample['extraversion'],\n",
    "            sample['agreeableness'],\n",
    "            sample['neuroticism']\n",
    "        ]).float()\n",
    "\n",
    "        return audio, vision, text, labels\n",
    "\n",
    "class PersonalityDatasetGen(Dataset):\n",
    "    def __init__(self, pkl_file):\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "        self.keys = list(self.data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "        sample = self.data[key]\n",
    "\n",
    "        audio = sample.get('original_audio', None)\n",
    "        if audio == None: \n",
    "            print(\"audio for\", idx, \"is None\")\n",
    "            new_idx = (idx + 1) % len(self.keys)\n",
    "            return self.__getitem__(new_idx)\n",
    "\n",
    "        if audio.shape[-1]!=5: \n",
    "            new_idx = (idx + 1) % len(self.keys)\n",
    "            return self.__getitem__(new_idx)\n",
    "\n",
    "            \n",
    "        if audio.ndim == 3: \n",
    "            audio = audio.mean(dim = 1)   \n",
    "        audio = audio.float()\n",
    "        audio = audio.transpose(0, 1)\n",
    "\n",
    "\n",
    "        vision = sample.get('reconstructed_visual', None)\n",
    "        if vision == None: \n",
    "            print(\"vision for\", idx, \"is None\")\n",
    "            new_idx = (idx + 1) % len(self.keys)\n",
    "            return self.__getitem__(new_idx)\n",
    "\n",
    "        if vision.shape[0]!=5: \n",
    "            new_idx = (idx + 1) % len(self.keys)\n",
    "            return self.__getitem__(new_idx)\n",
    "        if vision.ndim == 3: \n",
    "            vision = vision.squeeze(1) \n",
    "        vision = vision.float()\n",
    "\n",
    "        \n",
    "        text = sample['original_text'].unsqueeze(0).repeat(5, 1).float()\n",
    "\n",
    "\n",
    "        labels = torch.tensor([\n",
    "            sample['openness'],\n",
    "            sample['conscientiousness'],\n",
    "            sample['extraversion'],\n",
    "            sample['agreeableness'],\n",
    "            sample['neuroticism']\n",
    "        ]).float()\n",
    "\n",
    "        return audio, vision, text, labels\n",
    "\n",
    "class PersonalityDataset(Dataset):\n",
    "    def __init__(self, pkl_file):\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "\n",
    "        with open(pkl_file, 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "        self.keys = list(self.data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key = self.keys[idx]\n",
    "        sample = self.data[key]\n",
    "        print(sample)\n",
    "\n",
    "        audio = sample['original_audio']\n",
    "        if audio.shape[-1]!=5: \n",
    "            new_idx = (idx + 1) % len(self.keys)\n",
    "            return self.__getitem__(new_idx)\n",
    "\n",
    "        if audio.ndim == 3: \n",
    "            audio = audio.mean(dim=1) \n",
    "        audio = audio.float()\n",
    "        audio = audio.transpose(0, 1)\n",
    "\n",
    "\n",
    "        \n",
    "        vision = sample['original_vision']\n",
    "        if vision.shape[-1]!=5: \n",
    "            new_idx = (idx + 1) % len(self.keys)\n",
    "            return self.__getitem__(new_idx)\n",
    "        if vision.ndim == 3:  # (5, 5, 512) ?\n",
    "            vision = vision.mean(dim=1)  # (5, 512)\n",
    "        vision = vision.float()\n",
    "        vision = vision.transpose(0, 1)\n",
    "        text = sample['original_text'].unsqueeze(0).repeat(5, 1).float()\n",
    "\n",
    "        labels = torch.tensor([\n",
    "            sample['openness'],\n",
    "            sample['conscientiousness'],\n",
    "            sample['extraversion'],\n",
    "            sample['agreeableness'],\n",
    "            sample['neuroticism']\n",
    "        ]).float()\n",
    "\n",
    "        return audio, vision, text, labels\n",
    "\n",
    "def concordance_correlation_coefficient(y_true, y_pred):\n",
    "    mean_true = np.mean(y_true)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    var_true = np.var(y_true)\n",
    "    var_pred = np.var(y_pred)\n",
    "    cov = np.mean((y_true - mean_true) * (y_pred - mean_pred))\n",
    "\n",
    "    ccc = (2 * cov) / (var_true + var_pred + (mean_true - mean_pred) ** 2 + 1e-8)\n",
    "    return ccc\n",
    "\n",
    "\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for audio, vision, text, labels in loader:\n",
    "        # print(audio.shape)  # should be (batch_size, 5, 1024)\n",
    "        # print(vision.shape) # should be (batch_size, 5, 512)\n",
    "        # print(text.shape)   # should be (batch_size, 5, 768)\n",
    "        \n",
    "        audio, vision, text, labels = audio.to(device), vision.to(device), text.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(audio, vision, text)\n",
    "        loss = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for audio, vision, text, labels in loader:\n",
    "        audio, vision, text, labels = audio.to(device), vision.to(device), text.to(device), labels.to(device)\n",
    "        preds = model(audio, vision, text)\n",
    "        y_true.append(labels.cpu().numpy())\n",
    "        y_pred.append(preds.cpu().numpy())\n",
    "\n",
    "    y_true = np.vstack(y_true)\n",
    "    y_pred = np.vstack(y_pred)\n",
    "\n",
    "    trait_names = [\"Openness\", \"Conscientiousness\", \"Extraversion\", \"Agreeableness\", \"Neuroticism\"]\n",
    "    for i in range(5):\n",
    "        mse = mean_squared_error(y_true[:, i], y_pred[:, i])\n",
    "        mae = mean_absolute_error(y_true[:, i], y_pred[:, i])\n",
    "        acc = 1 - mae\n",
    "        ccc = concordance_correlation_coefficient(y_true[:, i], y_pred[:, i])\n",
    "        r2 = r2_score(y_true[:, i], y_pred[:, i])\n",
    "        pcc, _ = pearsonr(y_true[:, i], y_pred[:, i])\n",
    "\n",
    "        print(f\">> {trait_names[i]}:  MAE: {mae:.4f}  ACC: {acc:.4f}  MSE: {mse:.4f}  R2: {r2:.4f}  PCC: {pcc:.4f}  CCC: {ccc:.4f}\")\n",
    "    avg_ccc = np.mean([concordance_correlation_coefficient(y_true[:, i], y_pred[:, i]) for i in range(5)])\n",
    "    avg_r2 = np.mean([r2_score(y_true[:, i], y_pred[:, i]) for i in range(5)])\n",
    "    return avg_ccc, avg_r2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9024e84-3eac-480d-ba89-0265387749c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    train_pickle = '/project/msoleyma_1026/personality_detection/first_impressions_v2_dataset/reconstructed_cvae_train.pkl'\n",
    "    val_pickle = '/project/msoleyma_1026/personality_detection/first_impressions_v2_dataset/reconstructed_cvae_val.pkl'\n",
    "    test_pickle = '/project/msoleyma_1026/personality_detection/first_impressions_v2_dataset/cvae_test.pkl'\n",
    "\n",
    "\n",
    "    train_dataset = PersonalityDatasetGen(train_pickle)\n",
    "    val_dataset = PersonalityDatasetGen(val_pickle)\n",
    "    test_dataset = PersonalityDatasetGenTest(test_pickle)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "    model = TransformerFusionModel().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_val_ccc = -np.inf\n",
    "    patience = 5\n",
    "    wait = 0\n",
    "\n",
    "    for epoch in trange(30, desc=\"Epochs\"):\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "        val_ccc, val_r2 = evaluate(model, val_loader, device)\n",
    "        print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val CCC = {val_ccc:.4f}, Val R2 = {val_r2:.4f}\")\n",
    "\n",
    "        if val_ccc > best_val_ccc:\n",
    "            best_val_ccc = val_ccc\n",
    "            wait = 0\n",
    "            torch.save(model.state_dict(), 'best_transformer_model_gen_vid_original_aud.pth')\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}. Best Val CCC = {best_val_ccc:.4f}\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load('best_transformer_model_gen_vid_original_aud.pth'))\n",
    "    test_ccc, test_r2 = evaluate(model, test_loader, device)\n",
    "    print(f\"Test CCC = {test_ccc:.4f}, Test R2 = {test_r2:.4f}\")\n",
    "\n",
    " \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92828dd6-6510-4f96-8384-f322f8e75351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7976e1-8a16-4a93-b3ed-51f1e8a90d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0659d2e-e719-4bbe-919e-db647a4a210f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
