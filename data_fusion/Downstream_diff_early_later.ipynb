{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de1e6f5d-0ecb-40b3-804f-9a859c18a59c",
      "metadata": {
        "id": "de1e6f5d-0ecb-40b3-804f-9a859c18a59c",
        "outputId": "fad15b20-7a4e-45db-afe2-5a3328f7719a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-level type: <class 'list'>\n",
            "Number of entries: 1986\n",
            "\n",
            "Entry 1:\n",
            "Type: <class 'dict'>\n",
            "Keys: dict_keys(['original_text', 'original_vision', 'original_audio', 'ground_truth_text', 'ground_truth_vision', 'ground_truth_audio', 'generated_vision', 'generated_audio', 'agreeableness', 'openness', 'neuroticism', 'extraversion', 'conscientiousness'])\n",
            " - original_text: shape torch.Size([768, 5])\n",
            " - original_vision: shape torch.Size([512, 5])\n",
            " - original_audio: shape torch.Size([1024, 5])\n",
            " - ground_truth_text: shape torch.Size([32, 3])\n",
            " - ground_truth_vision: shape torch.Size([32, 3])\n",
            " - ground_truth_audio: shape torch.Size([32, 3])\n",
            " - generated_vision: shape torch.Size([32, 3])\n",
            " - generated_audio: shape torch.Size([32, 3])\n",
            " - agreeableness: 0.36263737082481384\n",
            " - openness: 0.47777777910232544\n",
            " - neuroticism: 0.3645833432674408\n",
            " - extraversion: 0.20560747385025024\n",
            " - conscientiousness: 0.33980581164360046\n",
            "\n",
            "Entry 2:\n",
            "Type: <class 'dict'>\n",
            "Keys: dict_keys(['original_text', 'original_vision', 'original_audio', 'ground_truth_text', 'ground_truth_vision', 'ground_truth_audio', 'generated_vision', 'generated_audio', 'agreeableness', 'openness', 'neuroticism', 'extraversion', 'conscientiousness'])\n",
            " - original_text: shape torch.Size([768, 5])\n",
            " - original_vision: shape torch.Size([512, 5])\n",
            " - original_audio: shape torch.Size([1024, 5])\n",
            " - ground_truth_text: shape torch.Size([32, 3])\n",
            " - ground_truth_vision: shape torch.Size([32, 3])\n",
            " - ground_truth_audio: shape torch.Size([32, 3])\n",
            " - generated_vision: shape torch.Size([32, 3])\n",
            " - generated_audio: shape torch.Size([32, 3])\n",
            " - agreeableness: 0.6043956279754639\n",
            " - openness: 0.6555555462837219\n",
            " - neuroticism: 0.4895833432674408\n",
            " - extraversion: 0.5327102541923523\n",
            " - conscientiousness: 0.5339806079864502\n",
            "\n",
            "Entry 3:\n",
            "Type: <class 'dict'>\n",
            "Keys: dict_keys(['original_text', 'original_vision', 'original_audio', 'ground_truth_text', 'ground_truth_vision', 'ground_truth_audio', 'generated_vision', 'generated_audio', 'agreeableness', 'openness', 'neuroticism', 'extraversion', 'conscientiousness'])\n",
            " - original_text: shape torch.Size([768, 5])\n",
            " - original_vision: shape torch.Size([512, 5])\n",
            " - original_audio: shape torch.Size([1024, 5])\n",
            " - ground_truth_text: shape torch.Size([32, 3])\n",
            " - ground_truth_vision: shape torch.Size([32, 3])\n",
            " - ground_truth_audio: shape torch.Size([32, 3])\n",
            " - generated_vision: shape torch.Size([32, 3])\n",
            " - generated_audio: shape torch.Size([32, 3])\n",
            " - agreeableness: 0.48351648449897766\n",
            " - openness: 0.31111112236976624\n",
            " - neuroticism: 0.3541666567325592\n",
            " - extraversion: 0.2897196114063263\n",
            " - conscientiousness: 0.3883495032787323\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Path\n",
        "pkl_path = '/project/msoleyma_1026/personality_detection/first_impressions_v2_dataset/reconstructed_outputs.pkl'\n",
        "\n",
        "# Load\n",
        "with open(pkl_path, 'rb') as f:\n",
        "    reconstructed_data = pickle.load(f)\n",
        "\n",
        "# Check type\n",
        "print(\"Top-level type:\", type(reconstructed_data))\n",
        "print(\"Number of entries:\", len(reconstructed_data))\n",
        "\n",
        "# Print first 3 entries safely\n",
        "for idx, entry in enumerate(reconstructed_data[:3]):\n",
        "    print(f\"\\nEntry {idx + 1}:\")\n",
        "    print(\"Type:\", type(entry))\n",
        "    if isinstance(entry, dict):\n",
        "        print(\"Keys:\", entry.keys())\n",
        "        for k, v in entry.items():\n",
        "            if hasattr(v, 'shape'):\n",
        "                print(f\" - {k}: shape {v.shape}\")\n",
        "            else:\n",
        "                print(f\" - {k}: {v}\")\n",
        "    else:\n",
        "        print(entry)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e52b8392-5b84-4c14-9a50-53ccaeabf627",
      "metadata": {
        "id": "e52b8392-5b84-4c14-9a50-53ccaeabf627"
      },
      "outputs": [],
      "source": [
        "with open('annotation_test.pkl', 'rb') as f:\n",
        "    annotations = pickle.load(f, encoding='latin1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4835786c-2c02-42a1-a556-e7c18b72be62",
      "metadata": {
        "id": "4835786c-2c02-42a1-a556-e7c18b72be62",
        "outputId": "4c5d5d52-49a7-4717-e25b-c9662fce55b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['extraversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'interview', 'openness'])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "annotations.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fdc4f47-0ef7-46ea-92c0-9086c25af7b7",
      "metadata": {
        "id": "3fdc4f47-0ef7-46ea-92c0-9086c25af7b7"
      },
      "outputs": [],
      "source": [
        "# annotations['openness']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f7b6190-def4-4f5e-9d18-f436a315dc63",
      "metadata": {
        "id": "1f7b6190-def4-4f5e-9d18-f436a315dc63",
        "outputId": "31061c12-ff7a-4ee3-a9a7-de5140057c6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Train Text Files (6000 files) ---\n",
            "cpzY1b6wJqs.002.npy\n",
            "Wx_oe0SxD9w.002.npy\n",
            "m04e9ylCoK0.003.npy\n",
            "e07IozLUeKc.005.npy\n",
            "z-CV743owek.005.npy\n",
            "\n",
            "--- Validation Text Files (2000 files) ---\n",
            "sPMNhG1Sehc.001.npy\n",
            "9rF3BEXetOo.003.npy\n",
            "lxnV9X8T2Zc.005.npy\n",
            "R-qB2FX7ZbE.002.npy\n",
            "mlXZQ8dO0nQ.001.npy\n",
            "\n",
            "--- Test Text Files (2000 files) ---\n",
            "XJj34u5IzU0.005.npy\n",
            "2GHz8LYflE0.003.npy\n",
            "uYvDMWWq2Jw.003.npy\n",
            "QXFRE_pjrAE.002.npy\n",
            "kFak4VnRnRM.000.npy\n",
            "\n",
            "--- Reconstructed Visual Files ---\n",
            "F5kL7RWS_f0.005.npy\n",
            "L4uFD6434Pc.000.npy\n",
            "XJj34u5IzU0.005.npy\n",
            "LP5N5uPfTdA.003.npy\n",
            "_tq-VgoXGMo.004.npy\n",
            "\n",
            "--- Reconstructed Audio Files ---\n",
            "RbX4q4KceVk.001.npy\n",
            "jvHDFrgu9PA.002.npy\n",
            "VPFrKx72gvo.003.npy\n",
            "xgwiqo2AsCA.004.npy\n",
            "p5v75vAZ7F0.000.npy\n",
            "\n",
            "--- Training Annotations (6000 video ids) ---\n",
            "Video ID: BZ3FEf_KKso.001.mp4 → Openness: 0.5333333333333333\n",
            "Video ID: PmJw3uI1qBE.000.mp4 → Openness: 0.3\n",
            "Video ID: 0mym1CooiTE.003.mp4 → Openness: 0.7888888888888888\n",
            "Video ID: OMHlfDF99Mw.000.mp4 → Openness: 0.6333333333333333\n",
            "Video ID: jub-AHFTH_g.000.mp4 → Openness: 0.6555555555555556\n",
            "\n",
            "--- Validation Annotations (2000 video ids) ---\n",
            "Video ID: 2d6btbaNdfo.001.mp4 → Openness: 0.5111111111111111\n",
            "Video ID: _7s27dUoYVg.004.mp4 → Openness: 0.7333333333333333\n",
            "Video ID: zupxsNi2er8.003.mp4 → Openness: 0.4\n",
            "Video ID: bFwtVtZodIg.003.mp4 → Openness: 0.7777777777777777\n",
            "Video ID: CFK8ib0aWe8.005.mp4 → Openness: 0.45555555555555555\n",
            "\n",
            "--- Test Annotations (2000 video ids) ---\n",
            "Video ID: 7Y3S4nfQHeo.003.mp4 → Openness: 0.5111111111111111\n",
            "Video ID: PReOtefm17s.004.mp4 → Openness: 0.7999999999999999\n",
            "Video ID: 3JAFkk42zPs.003.mp4 → Openness: 0.3777777777777778\n",
            "Video ID: d9_YDJdF-7I.001.mp4 → Openness: 0.41111111111111115\n",
            "Video ID: 2GHz8LYflE0.003.mp4 → Openness: 0.5111111111111111\n",
            "\n",
            "Trying to match Feature 'mpXOSY5dW7c.001' with Annotation 'mpXOSY5dW7c.001.mp4':\n",
            "  Openness: 0.5666666666666667\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "# Base paths\n",
        "base_text_dir = '/project/msoleyma_1026/personality_detection/first_impressions_v2_dataset/text_feature_vectors/'\n",
        "reconstructed_visual_dir = './reconstructed_visual_feature_vectors/'\n",
        "reconstructed_audio_dir = './reconstructed_audio_feature_vectors/'\n",
        "\n",
        "# Annotation files\n",
        "annotation_train_file = './annotation_training.pkl'\n",
        "annotation_val_file = './annotation_validation.pkl'\n",
        "annotation_test_file = './annotation_test.pkl'\n",
        "\n",
        "# Dirs\n",
        "train_dirs = [f\"train-{i}\" for i in range(1, 7)]\n",
        "val_dir = 'val'\n",
        "test_dir = 'test'\n",
        "\n",
        "def list_npy_files(directory):\n",
        "    if os.path.exists(directory):\n",
        "        return [f for f in os.listdir(directory) if f.endswith('.npy')]\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "def print_sample_files(base_dir, subdirs, label):\n",
        "    all_files = []\n",
        "    for sub in subdirs:\n",
        "        full_path = os.path.join(base_dir, sub)\n",
        "        all_files.extend(list_npy_files(full_path))\n",
        "    print(f\"\\n--- {label} ({len(all_files)} files) ---\")\n",
        "    for f in random.sample(all_files, min(5, len(all_files))):\n",
        "        print(f)\n",
        "\n",
        "def print_annotation_keys(annotation_file, label):\n",
        "    if os.path.exists(annotation_file):\n",
        "        with open(annotation_file, 'rb') as f:\n",
        "            annotations = pickle.load(f, encoding='latin1')\n",
        "        # Pick one trait to sample keys from\n",
        "        sample_trait = 'openness'\n",
        "        video_ids = list(annotations[sample_trait].keys())\n",
        "        print(f\"\\n--- {label} ({len(video_ids)} video ids) ---\")\n",
        "        for vid in random.sample(video_ids, min(5, len(video_ids))):\n",
        "            print(f\"Video ID: {vid} → Openness: {annotations[sample_trait][vid]}\")\n",
        "    else:\n",
        "        print(f\"Annotation file {annotation_file} not found!\")\n",
        "\n",
        "def test_lookup(annotation_file, feature_file_sample):\n",
        "    with open(annotation_file, 'rb') as f:\n",
        "        annotations = pickle.load(f, encoding='latin1')\n",
        "    feature_id = feature_file_sample.replace('.npy', '')\n",
        "    annotation_id = feature_id + '.mp4'\n",
        "    sample_trait = 'openness'\n",
        "    value = annotations[sample_trait].get(annotation_id, None)\n",
        "    print(f\"\\nTrying to match Feature '{feature_id}' with Annotation '{annotation_id}':\")\n",
        "    print(f\"  Openness: {value}\")\n",
        "\n",
        "# Print random samples\n",
        "print_sample_files(base_text_dir, train_dirs, \"Train Text Files\")\n",
        "print_sample_files(base_text_dir, [val_dir], \"Validation Text Files\")\n",
        "print_sample_files(base_text_dir, [test_dir], \"Test Text Files\")\n",
        "\n",
        "print(\"\\n--- Reconstructed Visual Files ---\")\n",
        "reconstructed_visual_files = list_npy_files(reconstructed_visual_dir)\n",
        "for f in random.sample(reconstructed_visual_files, min(5, len(reconstructed_visual_files))):\n",
        "    print(f)\n",
        "\n",
        "print(\"\\n--- Reconstructed Audio Files ---\")\n",
        "reconstructed_audio_files = list_npy_files(reconstructed_audio_dir)\n",
        "for f in random.sample(reconstructed_audio_files, min(5, len(reconstructed_audio_files))):\n",
        "    print(f)\n",
        "\n",
        "print_annotation_keys(annotation_train_file, \"Training Annotations\")\n",
        "print_annotation_keys(annotation_val_file, \"Validation Annotations\")\n",
        "print_annotation_keys(annotation_test_file, \"Test Annotations\")\n",
        "\n",
        "# Test matching a random test file\n",
        "if reconstructed_visual_files:\n",
        "    test_lookup(annotation_test_file, random.choice(reconstructed_visual_files))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f76f1c56-c617-4bea-b5ad-ae72590de3f4",
      "metadata": {
        "id": "f76f1c56-c617-4bea-b5ad-ae72590de3f4",
        "outputId": "9c756804-06ce-4b95-8c63-2f536d976df8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing train set: 6000 total samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/6000 [00:00<?, ?it/s]/tmp/SLURM_250782/ipykernel_3061271/170239581.py:44: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3725.)\n",
            "  return torch.tensor(arr).T  # Shape: (feature_dim, 5)\n",
            "100%|██████████| 6000/6000 [16:35<00:00,  6.02it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples processed: 6000 (Skipped 0)\n",
            "Saved pickle: cvae_train.pkl (6000 entries)\n",
            "\n",
            "Processing val set: 2000 total samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [06:02<00:00,  5.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val samples processed: 2000 (Skipped 0)\n",
            "Saved pickle: cvae_val.pkl (2000 entries)\n",
            "\n",
            "Processing test set: 2000 total samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [06:27<00:00,  5.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test samples processed: 1999 (Skipped 1)\n",
            "Saved pickle: cvae_test.pkl (1999 entries)\n",
            "\n",
            "✅ All pickle files created successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "base_dir = '/project/msoleyma_1026/personality_detection/first_impressions_v2_dataset/'\n",
        "\n",
        "text_dir = os.path.join(base_dir, 'text_feature_vectors')\n",
        "vision_dir = os.path.join(base_dir, 'video_feature_vectors')\n",
        "audio_dir = os.path.join(base_dir, 'audio_feature_vectors')\n",
        "\n",
        "reconstructed_visual_dir = './reconstructed_visual_feature_vectors/'\n",
        "reconstructed_audio_dir = './reconstructed_audio_feature_vectors/'\n",
        "\n",
        "annotation_train_file = './annotation_training.pkl'\n",
        "annotation_val_file = './annotation_validation.pkl'\n",
        "annotation_test_file = './annotation_test.pkl'\n",
        "\n",
        "train_subdirs = [f\"train-{i}\" for i in range(1, 7)]\n",
        "val_subdirs = ['val']\n",
        "test_subdirs = ['test']\n",
        "\n",
        "\n",
        "def load_annotations(annotation_path):\n",
        "    with open(annotation_path, 'rb') as f:\n",
        "        annotations = pickle.load(f, encoding='latin1')\n",
        "    return annotations\n",
        "\n",
        "def collect_feature_files(base_path, subdirs):\n",
        "    npy_files = []\n",
        "    for subdir in subdirs:\n",
        "        full_path = os.path.join(base_path, subdir)\n",
        "        if os.path.exists(full_path):\n",
        "            files = [os.path.join(full_path, f) for f in os.listdir(full_path) if f.endswith('.npy')]\n",
        "            npy_files.extend(files)\n",
        "    return npy_files\n",
        "\n",
        "def load_feature(path):\n",
        "    if os.path.exists(path):\n",
        "        arr = np.load(path)\n",
        "        return torch.tensor(arr).T  # Shape: (feature_dim, 5)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def lookup_traits(annotations, video_id_no_ext):\n",
        "    video_id_with_ext = video_id_no_ext + '.mp4'\n",
        "    traits = {}\n",
        "    try:\n",
        "        traits['agreeableness'] = annotations['agreeableness'][video_id_with_ext]\n",
        "        traits['openness'] = annotations['openness'][video_id_with_ext]\n",
        "        traits['neuroticism'] = annotations['neuroticism'][video_id_with_ext]\n",
        "        traits['extraversion'] = annotations['extraversion'][video_id_with_ext]\n",
        "        traits['conscientiousness'] = annotations['conscientiousness'][video_id_with_ext]\n",
        "    except KeyError:\n",
        "        return None\n",
        "    return traits\n",
        "\n",
        "def process_split(feature_dir_base, subdirs, annotations, split_type, recon_visual_dir=None, recon_audio_dir=None):\n",
        "    output = {}\n",
        "    feature_files = collect_feature_files(feature_dir_base, subdirs)\n",
        "    print(f\"\\nProcessing {split_type} set: {len(feature_files)} total samples...\")\n",
        "\n",
        "    skipped = 0\n",
        "    for file_path in tqdm(feature_files):\n",
        "        video_filename = os.path.basename(file_path)  # e.g., 'abc123.003.npy'\n",
        "        video_id = video_filename.replace('.npy', '')  # e.g., 'abc123.003'\n",
        "\n",
        "        subdir = file_path.split('/')[-2]\n",
        "        filename = file_path.split('/')[-1]\n",
        "\n",
        "        # Load original features\n",
        "        text_feature = load_feature(os.path.join(text_dir, subdir, filename))\n",
        "        vision_feature = load_feature(os.path.join(vision_dir, subdir, filename))\n",
        "        audio_feature = load_feature(os.path.join(audio_dir, subdir, filename))\n",
        "\n",
        "        if text_feature is None or vision_feature is None or audio_feature is None:\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        # Lookup traits\n",
        "        traits = lookup_traits(annotations, video_id)\n",
        "        if traits is None:\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        entry = {\n",
        "            'original_text': text_feature,\n",
        "            'original_vision': vision_feature,\n",
        "            'original_audio': audio_feature,\n",
        "            **traits\n",
        "        }\n",
        "\n",
        "        # Only for test set, load reconstructions if requested\n",
        "        if split_type == 'test' and recon_visual_dir and recon_audio_dir:\n",
        "            visual_recon_path = os.path.join(reconstructed_visual_dir, f\"{video_id}.npy\")\n",
        "            audio_recon_path = os.path.join(reconstructed_audio_dir, f\"{video_id}.npy\")\n",
        "\n",
        "            if os.path.exists(visual_recon_path) and os.path.exists(audio_recon_path):\n",
        "                entry['generated_vision'] = torch.tensor(np.load(visual_recon_path))  # (32,5)\n",
        "                entry['generated_audio'] = torch.tensor(np.load(audio_recon_path))    # (32,5)\n",
        "            else:\n",
        "                # Skip samples without both reconstructions\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "        output[video_id] = entry\n",
        "\n",
        "    print(f\"{split_type.capitalize()} samples processed: {len(output)} (Skipped {skipped})\")\n",
        "    return output\n",
        "\n",
        "def save_pickle(data, save_path):\n",
        "    with open(save_path, 'wb') as f:\n",
        "        pickle.dump(data, f)\n",
        "    print(f\"Saved pickle: {save_path} ({len(data)} entries)\")\n",
        "\n",
        "\n",
        "# Load annotation files\n",
        "annotations_train = load_annotations(annotation_train_file)\n",
        "annotations_val = load_annotations(annotation_val_file)\n",
        "annotations_test = load_annotations(annotation_test_file)\n",
        "\n",
        "# Process train\n",
        "train_data = process_split(\n",
        "    feature_dir_base=text_dir,\n",
        "    subdirs=train_subdirs,\n",
        "    annotations=annotations_train,\n",
        "    split_type='train'\n",
        ")\n",
        "save_pickle(train_data, 'cvae_train.pkl')\n",
        "\n",
        "# Process validation\n",
        "val_data = process_split(\n",
        "    feature_dir_base=text_dir,\n",
        "    subdirs=val_subdirs,\n",
        "    annotations=annotations_val,\n",
        "    split_type='val'\n",
        ")\n",
        "save_pickle(val_data, 'cvae_val.pkl')\n",
        "\n",
        "# Process test (with reconstructions)\n",
        "test_data = process_split(\n",
        "    feature_dir_base=text_dir,\n",
        "    subdirs=test_subdirs,\n",
        "    annotations=annotations_test,\n",
        "    split_type='test',\n",
        "    recon_visual_dir=reconstructed_visual_dir,\n",
        "    recon_audio_dir=reconstructed_audio_dir\n",
        ")\n",
        "save_pickle(test_data, 'cvae_test.pkl')\n",
        "\n",
        "print(\"\\n✅ All pickle files created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fbee005-be18-4637-83e7-62581c36dd87",
      "metadata": {
        "id": "5fbee005-be18-4637-83e7-62581c36dd87",
        "outputId": "d7e5c44f-3f1a-4b71-aaee-0be5ab3668e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Data Type: <class 'list'>\n",
            "Number of Train Samples: 5952\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'keys'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of Train Samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Peek inside one sample\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m first_key = \u001b[38;5;28mlist\u001b[39m(\u001b[43mtrain_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m())[\u001b[32m0\u001b[39m]\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSample Key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfirst_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSample Entry Type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(train_data[first_key])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'keys'"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Path to the files\n",
        "train_pkl = 'reconstructed_outputs_train.pkl'\n",
        "val_pkl = 'reconstructed_outputs_valid.pkl'\n",
        "test_pkl = 'reconstructed_outputs_test.pkl'\n",
        "\n",
        "# Load\n",
        "def load_pickle(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f, encoding='latin1')\n",
        "\n",
        "train_data = load_pickle(train_pkl)\n",
        "val_data = load_pickle(val_pkl)\n",
        "test_data = load_pickle(test_pkl)\n",
        "\n",
        "# Print top-level type\n",
        "print(f\"Train Data Type: {type(train_data)}\")\n",
        "print(f\"Number of Train Samples: {len(train_data)}\")\n",
        "\n",
        "# # Peek inside one sample\n",
        "# first_key = list(train_data.keys())[0]\n",
        "# print(f\"Sample Key: {first_key}\")\n",
        "# print(f\"Sample Entry Type: {type(train_data[first_key])}\")\n",
        "\n",
        "# # Print shapes if available\n",
        "# sample = train_data[first_key]\n",
        "# if isinstance(sample, dict):\n",
        "#     for k, v in sample.items():\n",
        "#         if hasattr(v, 'shape'):\n",
        "#             print(f\" - {k}: shape {v.shape}\")\n",
        "#         else:\n",
        "#             print(f\" - {k}: {type(v)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e94bb2c7-3c83-4e37-bf8a-905b14d67139",
      "metadata": {
        "id": "e94bb2c7-3c83-4e37-bf8a-905b14d67139",
        "outputId": "bb9c4ae4-7a92-476a-aec4-0c081ba013c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Type: <class 'dict'>\n",
            "Sample Keys: ['original_text', 'original_vision', 'original_audio', 'ground_truth_text', 'ground_truth_vision', 'ground_truth_audio', 'generated_vision', 'generated_audio', 'agreeableness', 'openness', 'neuroticism', 'extraversion', 'conscientiousness']\n",
            " - original_text: shape torch.Size([768, 5])\n",
            " - original_vision: shape torch.Size([512, 5])\n",
            " - original_audio: shape torch.Size([1024, 5])\n",
            " - ground_truth_text: shape torch.Size([32, 3])\n",
            " - ground_truth_vision: shape torch.Size([32, 3])\n",
            " - ground_truth_audio: shape torch.Size([32, 3])\n",
            " - generated_vision: shape torch.Size([32, 3])\n",
            " - generated_audio: shape torch.Size([32, 3])\n",
            " - agreeableness: <class 'float'>, value=0.5054945349693298\n",
            " - openness: <class 'float'>, value=0.5777778029441833\n",
            " - neuroticism: <class 'float'>, value=0.4479166567325592\n",
            " - extraversion: <class 'float'>, value=0.4392523467540741\n",
            " - conscientiousness: <class 'float'>, value=0.5631067752838135\n"
          ]
        }
      ],
      "source": [
        "# Pick a sample from the list\n",
        "sample = train_data[0]\n",
        "\n",
        "print(f\"Sample Type: {type(sample)}\")\n",
        "print(f\"Sample Keys: {list(sample.keys())}\")\n",
        "\n",
        "# Print types and shapes\n",
        "for k, v in sample.items():\n",
        "    if hasattr(v, 'shape'):\n",
        "        print(f\" - {k}: shape {v.shape}\")\n",
        "    else:\n",
        "        print(f\" - {k}: {type(v)}, value={v}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a33ba2c-8df0-4121-b695-f0f38c4a4181",
      "metadata": {
        "id": "6a33ba2c-8df0-4121-b695-f0f38c4a4181"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DiffusionDownstreamDataset(Dataset):\n",
        "    def __init__(self, data_list, split, task_type):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_list: loaded list from pickle (train/val/test)\n",
        "            split: 'train', 'val', 'test'\n",
        "            task_type: 'upper', 'middle_audio', 'middle_vision', 'lower_audio', 'lower_vision'\n",
        "        \"\"\"\n",
        "        self.data = data_list\n",
        "        self.split = split\n",
        "        self.task_type = task_type\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def mean_pool(self, feature):\n",
        "        if isinstance(feature, torch.Tensor):\n",
        "            return feature.view(-1)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported feature type: {type(feature)}\")\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        input_feats = {}\n",
        "\n",
        "        # Text (always ground truth)\n",
        "        text_feat = self.mean_pool(sample['ground_truth_text'])\n",
        "\n",
        "        # Vision\n",
        "        if self.split == 'test' and self.task_type in ['middle_audio', 'lower_audio']:\n",
        "            # Vision missing ➔ use generated_vision\n",
        "            vision_feat = self.mean_pool(sample['generated_vision'])\n",
        "        else:\n",
        "            # Vision available\n",
        "            vision_feat = self.mean_pool(sample['ground_truth_vision'])\n",
        "\n",
        "        # Audio\n",
        "        if self.split == 'test' and self.task_type in ['middle_vision', 'lower_vision']:\n",
        "            # Audio missing ➔ use generated_audio\n",
        "            audio_feat = self.mean_pool(sample['generated_audio'])\n",
        "        else:\n",
        "            # Audio available\n",
        "            audio_feat = self.mean_pool(sample['ground_truth_audio'])\n",
        "\n",
        "        # Assign based on task type\n",
        "        if self.task_type == 'upper':\n",
        "            input_feats = {'audio': audio_feat, 'vision': vision_feat, 'text': text_feat}\n",
        "        elif self.task_type == 'middle_audio':\n",
        "            input_feats = {'audio': audio_feat, 'vision': vision_feat, 'text': text_feat}\n",
        "        elif self.task_type == 'middle_vision':\n",
        "            input_feats = {'audio': audio_feat, 'vision': vision_feat, 'text': text_feat}\n",
        "        elif self.task_type == 'lower_audio':\n",
        "            input_feats = {'vision': vision_feat, 'text': text_feat}\n",
        "        elif self.task_type == 'lower_vision':\n",
        "            input_feats = {'audio': audio_feat, 'text': text_feat}\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown task type {self.task_type}\")\n",
        "\n",
        "        # Traits target\n",
        "        traits = torch.tensor([\n",
        "            sample['agreeableness'],\n",
        "            sample['openness'],\n",
        "            sample['neuroticism'],\n",
        "            sample['extraversion'],\n",
        "            sample['conscientiousness']\n",
        "        ], dtype=torch.float32)\n",
        "\n",
        "        return input_feats, traits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c676ef9e-b2fa-4733-82f0-a18b2f485426",
      "metadata": {
        "id": "c676ef9e-b2fa-4733-82f0-a18b2f485426"
      },
      "outputs": [],
      "source": [
        "class ModalityLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch, feature_dim)  or (batch, feature_dim, time_steps)\n",
        "        \"\"\"\n",
        "        if x.dim() == 3:\n",
        "            # (batch, feature_dim, time_steps) ➔ (batch, time_steps, feature_dim)\n",
        "            x = x.permute(0, 2, 1)\n",
        "        elif x.dim() == 2:\n",
        "            # (batch, feature_dim) ➔ unsqueeze to (batch, 1, feature_dim)\n",
        "            x = x.unsqueeze(1)\n",
        "\n",
        "        out, (h_n, c_n) = self.lstm(x)\n",
        "        return h_n[-1]  # Return last hidden state\n",
        "\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, input_dim=768, output_dim=256):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c560379-0f86-4d4e-8d90-b2c366a9841e",
      "metadata": {
        "id": "2c560379-0f86-4d4e-8d90-b2c366a9841e"
      },
      "outputs": [],
      "source": [
        "class EarlyFusionRegressor(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dim=256, output_dim=5):\n",
        "        super().__init__()\n",
        "        self.input_dims = input_dims\n",
        "        total_input_dim = sum(input_dims.values())\n",
        "        self.fc1 = nn.Linear(total_input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # inputs: dict of {'audio': tensor, 'vision': tensor, 'text': tensor}\n",
        "        x = torch.cat(list(inputs.values()), dim=-1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class LateFusionRegressor(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dim=256, output_dim=5):\n",
        "        super().__init__()\n",
        "        self.modalities = nn.ModuleDict({\n",
        "            k: nn.Sequential(\n",
        "                nn.Linear(dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, output_dim)\n",
        "            )\n",
        "            for k, dim in input_dims.items()\n",
        "        })\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        preds = []\n",
        "        for k, v in inputs.items():\n",
        "            preds.append(self.modalities[k](v))\n",
        "        # Mean the outputs\n",
        "        preds = torch.stack(preds, dim=0).mean(dim=0)\n",
        "        return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0a36f8b-315b-4c63-b489-5826367b76a7",
      "metadata": {
        "id": "b0a36f8b-315b-4c63-b489-5826367b76a7",
        "outputId": "c2e8a0dd-5250-458c-b7e2-b75bb737121d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Using cached scikit_learn-1.6.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in ./.conda/envs/remodiff/lib/python3.13/site-packages (from scikit-learn) (2.2.3)\n",
            "Collecting scipy>=1.6.0 (from scikit-learn)\n",
            "  Using cached scipy-1.15.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in ./.conda/envs/remodiff/lib/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Using cached scikit_learn-1.6.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.2 MB)\n",
            "Using cached scipy-1.15.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
            "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
            "Successfully installed scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f960f75b-3cc2-402b-b153-eb4c9ce279e4",
      "metadata": {
        "id": "f960f75b-3cc2-402b-b153-eb4c9ce279e4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from scipy.stats import pearsonr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80426f73-b81f-41aa-88e7-0f90b94fa7c3",
      "metadata": {
        "id": "80426f73-b81f-41aa-88e7-0f90b94fa7c3"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, optimizer, loss_fn=nn.MSELoss(), device='cuda'):\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.device = device\n",
        "\n",
        "    def train(self, train_loader, val_loader, epochs=50, patience=5):\n",
        "        best_val_loss = float('inf')\n",
        "        best_model = None\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            total_train_loss = 0\n",
        "            for batch_x, batch_y in train_loader:\n",
        "                batch_x = {k: v.to(self.device) for k, v in batch_x.items()}\n",
        "                batch_y = batch_y.to(self.device)\n",
        "\n",
        "                preds = self.model(batch_x)\n",
        "                loss = self.loss_fn(preds, batch_y)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "            val_loss = self.evaluate_loss(val_loader)\n",
        "            print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model = self.model.state_dict()\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered!\")\n",
        "                break\n",
        "\n",
        "        self.model.load_state_dict(best_model)\n",
        "\n",
        "    def evaluate_loss(self, loader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in loader:\n",
        "                batch_x = {k: v.to(self.device) for k, v in batch_x.items()}\n",
        "                batch_y = batch_y.to(self.device)\n",
        "\n",
        "                preds = self.model(batch_x)\n",
        "                loss = self.loss_fn(preds, batch_y)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf34ad50-1e68-4005-ac31-33dd0f627e53",
      "metadata": {
        "id": "cf34ad50-1e68-4005-ac31-33dd0f627e53"
      },
      "outputs": [],
      "source": [
        "class Evaluator:\n",
        "    @staticmethod\n",
        "    def evaluate(model, loader, device='cuda'):\n",
        "        model.eval()\n",
        "        preds_list, labels_list = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in loader:\n",
        "                batch_x = {k: v.to(device) for k, v in batch_x.items()}\n",
        "                batch_y = batch_y.to(device)\n",
        "\n",
        "                preds = model(batch_x)\n",
        "                preds_list.append(preds.cpu())\n",
        "                labels_list.append(batch_y.cpu())\n",
        "\n",
        "        preds = torch.cat(preds_list, dim=0).numpy()\n",
        "        labels = torch.cat(labels_list, dim=0).numpy()\n",
        "\n",
        "        return Evaluator.compute_metrics(preds, labels)\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_metrics(preds, labels):\n",
        "        results = {}\n",
        "        num_traits = preds.shape[1]\n",
        "\n",
        "        for i in range(num_traits):\n",
        "            p = preds[:, i]\n",
        "            l = labels[:, i]\n",
        "\n",
        "            mae = np.mean(np.abs(p - l))\n",
        "            acc = 1 - mae\n",
        "            mse = mean_squared_error(l, p)\n",
        "            r2 = r2_score(l, p)\n",
        "            pcc, _ = pearsonr(l, p)\n",
        "            mean_p = np.mean(p)\n",
        "            mean_l = np.mean(l)\n",
        "            var_p = np.var(p)\n",
        "            var_l = np.var(l)\n",
        "            ccc = (2 * pcc * np.sqrt(var_p) * np.sqrt(var_l)) / (var_p + var_l + (mean_p - mean_l) ** 2)\n",
        "\n",
        "            results[f'trait_{i+1}'] = {\n",
        "                'MAE': mae,\n",
        "                'ACC': acc,\n",
        "                'MSE': mse,\n",
        "                'R2': r2,\n",
        "                'PCC': pcc,\n",
        "                'CCC': ccc\n",
        "            }\n",
        "\n",
        "        # Aggregate metrics\n",
        "        avg_metrics = {metric: np.mean([results[f'trait_{i+1}'][metric] for i in range(num_traits)]) for metric in ['MAE', 'ACC', 'MSE', 'R2', 'PCC', 'CCC']}\n",
        "        results['average'] = avg_metrics\n",
        "\n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7e3d574-4ddf-4e59-baaf-0a4c15df2d09",
      "metadata": {
        "id": "b7e3d574-4ddf-4e59-baaf-0a4c15df2d09"
      },
      "outputs": [],
      "source": [
        "class ExperimentRunner:\n",
        "    def __init__(self, train_data, val_data, test_data, batch_size=64, device='cuda', dataset_cls=None):\n",
        "        self.train_data = train_data\n",
        "        self.val_data = val_data\n",
        "        self.test_data = test_data\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.dataset_cls = dataset_cls if dataset_cls is not None else PersonalityDataset  # default is PersonalityDataset\n",
        "\n",
        "    def run(self, task_type, fusion_type):\n",
        "        print(f\"\\n=== Running Task: {task_type.upper()}, Fusion: {fusion_type.upper()} ===\")\n",
        "\n",
        "        # Use the custom dataset class\n",
        "        train_dataset = self.dataset_cls(self.train_data, split='train', task_type=task_type)\n",
        "        val_dataset = self.dataset_cls(self.val_data, split='val', task_type=task_type)\n",
        "        test_dataset = self.dataset_cls(self.test_data, split='test', task_type=task_type)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "        input_dims = self.get_input_dims(train_dataset)\n",
        "\n",
        "        audio_lstm = ModalityLSTM(96, hidden_dim=256) if 'audio' in input_dims else None\n",
        "        vision_lstm = ModalityLSTM(96, hidden_dim=256) if 'vision' in input_dims else None\n",
        "        text_encoder = TextEncoder(96, output_dim=256)\n",
        "\n",
        "\n",
        "        if fusion_type == 'early':\n",
        "            model = EarlyFusionRegressor(input_dims={k: 256 for k in input_dims})\n",
        "        else:\n",
        "            model = LateFusionRegressor(input_dims={k: 256 for k in input_dims})\n",
        "\n",
        "        full_model = FullModel(audio_lstm, vision_lstm, text_encoder, model, device=self.device)\n",
        "        optimizer = torch.optim.Adam(full_model.parameters(), lr=1e-3)\n",
        "\n",
        "        trainer = Trainer(full_model, optimizer, loss_fn=nn.MSELoss(), device=self.device)\n",
        "        trainer.train(train_loader, val_loader, epochs=50, patience=5)\n",
        "\n",
        "        evaluator = Evaluator()\n",
        "        results = evaluator.evaluate(full_model, test_loader, device=self.device)\n",
        "\n",
        "        print(\"Results:\")\n",
        "        for trait, metrics in results.items():\n",
        "            print(f\"{trait}: {metrics}\")\n",
        "\n",
        "        return results  # very important: return results!\n",
        "\n",
        "    def get_input_dims(self, dataset):\n",
        "        sample, _ = dataset[0]\n",
        "        return {k: v.shape[-1] for k, v in sample.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d3e072f-3e22-40f4-828c-13e2d1df7282",
      "metadata": {
        "id": "8d3e072f-3e22-40f4-828c-13e2d1df7282"
      },
      "outputs": [],
      "source": [
        "class FullModel(nn.Module):\n",
        "    def __init__(self, audio_lstm, vision_lstm, text_encoder, fusion_model, device='cuda'):\n",
        "        super().__init__()\n",
        "        self.audio_lstm = audio_lstm.to(device) if audio_lstm else None\n",
        "        self.vision_lstm = vision_lstm.to(device) if vision_lstm else None\n",
        "        self.text_encoder = text_encoder.to(device)\n",
        "        self.fusion_model = fusion_model.to(device)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        feats = {}\n",
        "        if 'audio' in inputs and self.audio_lstm:\n",
        "            feats['audio'] = self.audio_lstm(inputs['audio'])\n",
        "        if 'vision' in inputs and self.vision_lstm:\n",
        "            feats['vision'] = self.vision_lstm(inputs['vision'])\n",
        "        if 'text' in inputs:\n",
        "            feats['text'] = self.text_encoder(inputs['text'])\n",
        "\n",
        "        return self.fusion_model(feats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eef96256-0e6a-4e26-8f77-32ef9c342c60",
      "metadata": {
        "id": "eef96256-0e6a-4e26-8f77-32ef9c342c60"
      },
      "outputs": [],
      "source": [
        "runner = ExperimentRunner(\n",
        "    train_data=train_data,\n",
        "    val_data=val_data,\n",
        "    test_data=test_data,\n",
        "    batch_size=64,\n",
        "    device='cuda',\n",
        "    dataset_cls=DiffusionDownstreamDataset  # <-- new dataset class for latent\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fb5e1cf0-0eb4-407f-a0d3-48b8b0aa5864",
      "metadata": {
        "id": "fb5e1cf0-0eb4-407f-a0d3-48b8b0aa5864"
      },
      "outputs": [],
      "source": [
        "task = 'middle_audio'\n",
        "fusion = 'early'\n",
        "results = runner.run(task, fusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "700e2e78-dd26-4429-b42d-6d57b2315dd3",
      "metadata": {
        "id": "700e2e78-dd26-4429-b42d-6d57b2315dd3"
      },
      "outputs": [],
      "source": [
        "# Middle Audio - Late Fusion\n",
        "task = 'middle_audio'\n",
        "fusion = 'late'\n",
        "results = runner.run(task, fusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "68609b92-e99c-4ee7-8967-006505d4f0b4",
      "metadata": {
        "id": "68609b92-e99c-4ee7-8967-006505d4f0b4"
      },
      "outputs": [],
      "source": [
        "# Middle Vision - Early Fusion\n",
        "task = 'middle_vision'\n",
        "fusion = 'early'\n",
        "results = runner.run(task, fusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "87809cea-f842-4348-aee8-b49df662d874",
      "metadata": {
        "id": "87809cea-f842-4348-aee8-b49df662d874"
      },
      "outputs": [],
      "source": [
        "# Middle Vision - Late Fusion\n",
        "task = 'middle_vision'\n",
        "fusion = 'late'\n",
        "results = runner.run(task, fusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3f9df2e-8318-454d-8678-051744f14f3c",
      "metadata": {
        "id": "a3f9df2e-8318-454d-8678-051744f14f3c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "remodiff-kernel",
      "language": "python",
      "name": "remodiff-kernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}