{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb10b70e-de04-4bf4-988a-9e88a6e31309",
      "metadata": {
        "id": "cb10b70e-de04-4bf4-988a-9e88a6e31309",
        "outputId": "90b3b710-bd7f-4ff5-fed0-148f58c712f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- TRAIN (6000 samples) ---\n",
            "Video ID: FxVUG2R1y0Q.004\n",
            "  original_text shape: torch.Size([768])\n",
            "  original_vision shape: torch.Size([512, 1, 5])\n",
            "  original_audio shape: torch.Size([1024, 149, 5])\n",
            "  Traits: Agreeableness 0.835164835164835, Openness 0.7999999999999999\n",
            "Video ID: TD3H2DOSi1Y.001\n",
            "  original_text shape: torch.Size([768])\n",
            "  original_vision shape: torch.Size([512, 1, 5])\n",
            "  original_audio shape: torch.Size([1024, 149, 5])\n",
            "  Traits: Agreeableness 0.6703296703296703, Openness 0.6777777777777777\n",
            "Video ID: wK_ExIjn5Q8.002\n",
            "  original_text shape: torch.Size([768])\n",
            "  original_vision shape: torch.Size([512, 1, 5])\n",
            "  original_audio shape: torch.Size([1024, 149, 5])\n",
            "  Traits: Agreeableness 0.4395604395604395, Openness 0.5444444444444444\n",
            "\n",
            "--- VALIDATION (2000 samples) ---\n",
            "Video ID: tvKUJujTUEo.002\n",
            "  original_text shape: torch.Size([768])\n",
            "  original_vision shape: torch.Size([512, 1, 5])\n",
            "  original_audio shape: torch.Size([1024, 149, 5])\n",
            "  Traits: Agreeableness 0.7032967032967032, Openness 0.6333333333333333\n",
            "Video ID: J5Q9cutAmY8.001\n",
            "  original_text shape: torch.Size([768])\n",
            "  original_vision shape: torch.Size([512, 1, 5])\n",
            "  original_audio shape: torch.Size([1024, 149, 5])\n",
            "  Traits: Agreeableness 0.6043956043956044, Openness 0.43333333333333335\n",
            "Video ID: dvU5jYExl0o.000\n",
            "  original_text shape: torch.Size([768])\n",
            "  original_vision shape: torch.Size([512, 1, 5])\n",
            "  original_audio shape: torch.Size([1024, 149, 5])\n",
            "  Traits: Agreeableness 0.5054945054945055, Openness 0.2777777777777778\n",
            "\n",
            "--- TEST (1999 samples) ---\n",
            "Video ID: oontew1LiUU.003\n",
            "  original_text shape: torch.Size([768])\n",
            "  original_vision shape: torch.Size([512, 1, 5])\n",
            "  original_audio shape: torch.Size([1024, 149, 5])\n",
            "  generated_vision shape: torch.Size([5, 1, 512])\n",
            "  generated_audio shape: torch.Size([5, 1, 1024])\n",
            "  Traits: Agreeableness 0.4285714285714285, Openness 0.4444444444444445\n",
            "Video ID: wTo1uZns2X8.002\n",
            "  original_text shape: torch.Size([768])\n",
            "  original_vision shape: torch.Size([512, 1, 5])\n",
            "  original_audio shape: torch.Size([1024, 149, 5])\n",
            "  generated_vision shape: torch.Size([5, 1, 512])\n",
            "  generated_audio shape: torch.Size([5, 1, 1024])\n",
            "  Traits: Agreeableness 0.4505494505494505, Openness 0.34444444444444444\n",
            "Video ID: 8hGOOynn3ZU.000\n",
            "  original_text shape: torch.Size([768])\n",
            "  original_vision shape: torch.Size([512, 1, 5])\n",
            "  original_audio shape: torch.Size([1024, 149, 5])\n",
            "  generated_vision shape: torch.Size([5, 1, 512])\n",
            "  generated_audio shape: torch.Size([5, 1, 1024])\n",
            "  Traits: Agreeableness 0.4395604395604395, Openness 0.38888888888888895\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import random\n",
        "import torch\n",
        "\n",
        "# Load function\n",
        "def load_pickle(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Paths\n",
        "train_pickle = 'cvae_train.pkl'\n",
        "val_pickle = 'cvae_val.pkl'\n",
        "test_pickle = 'cvae_test.pkl'\n",
        "\n",
        "# Load data\n",
        "train_data = load_pickle(train_pickle)\n",
        "val_data = load_pickle(val_pickle)\n",
        "test_data = load_pickle(test_pickle)\n",
        "\n",
        "# Print random few samples from each split\n",
        "def print_sample_shapes(data_dict, split_name, check_generated=False):\n",
        "    keys = list(data_dict.keys())\n",
        "    print(f\"\\n--- {split_name.upper()} ({len(keys)} samples) ---\")\n",
        "    for vid in random.sample(keys, min(3, len(keys))):\n",
        "        sample = data_dict[vid]\n",
        "        print(f\"Video ID: {vid}\")\n",
        "        print(f\"  original_text shape: {sample['original_text'].shape}\")\n",
        "        print(f\"  original_vision shape: {sample['original_vision'].shape}\")\n",
        "        print(f\"  original_audio shape: {sample['original_audio'].shape}\")\n",
        "        if check_generated:\n",
        "            if 'generated_vision' in sample:\n",
        "                print(f\"  generated_vision shape: {sample['generated_vision'].shape}\")\n",
        "            if 'generated_audio' in sample:\n",
        "                print(f\"  generated_audio shape: {sample['generated_audio'].shape}\")\n",
        "        print(f\"  Traits: Agreeableness {sample['agreeableness']}, Openness {sample['openness']}\")\n",
        "\n",
        "# Print examples\n",
        "print_sample_shapes(train_data, \"Train\")\n",
        "print_sample_shapes(val_data, \"Validation\")\n",
        "print_sample_shapes(test_data, \"Test\", check_generated=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daa26381-1a4f-4bb0-a204-177654768b80",
      "metadata": {
        "id": "daa26381-1a4f-4bb0-a204-177654768b80"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3ebc33c-e291-4ab7-b67d-14121269546f",
      "metadata": {
        "id": "a3ebc33c-e291-4ab7-b67d-14121269546f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "def pad_tensor_to_shape(x, target_shape):\n",
        "    \"\"\"Pads a tensor with zeros to match the target shape.\"\"\"\n",
        "    current_shape = x.shape\n",
        "    pad = []\n",
        "    for c, t in zip(reversed(current_shape), reversed(target_shape)):\n",
        "        pad.extend([0, max(t - c, 0)])\n",
        "    return F.pad(x, pad)\n",
        "\n",
        "class PersonalityDataset(Dataset):\n",
        "    def __init__(self, data_dict, split, task_type):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dict: dictionary from loaded pickle\n",
        "            split: 'train', 'val', 'test'\n",
        "            task_type: 'upper', 'middle_audio', 'middle_vision', 'lower_audio', 'lower_vision'\n",
        "        \"\"\"\n",
        "        self.data = data_dict\n",
        "        self.split = split\n",
        "        self.task_type = task_type\n",
        "        self.keys = list(data_dict.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.keys)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        vid = self.keys[idx]\n",
        "        sample = self.data[vid]\n",
        "\n",
        "        text_feat = sample['original_text'].float()\n",
        "\n",
        "        # -------- Handle audio feature --------\n",
        "        if self.split == 'test' and self.task_type in ['middle_vision', 'lower_vision']:\n",
        "            audio = sample['generated_audio'].squeeze(1).permute(1, 0).float()\n",
        "        else:\n",
        "            audio = sample['original_audio'].float()\n",
        "\n",
        "        # Fix audio shapes\n",
        "        if audio.dim() == 3:\n",
        "            audio_feat = audio.mean(dim=1)  # (1024, frames, 5) → (1024,5)\n",
        "        elif audio.dim() == 2:\n",
        "            audio_feat = audio  # (1024,5)\n",
        "        elif audio.dim() == 1:\n",
        "            audio_feat = audio.unsqueeze(1).repeat(1, 5)  # (1024,) → (1024,5)\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected audio feature shape {audio.shape} for video {vid}\")\n",
        "\n",
        "        audio_feat = pad_tensor_to_shape(audio_feat, (1024,5))\n",
        "\n",
        "        # -------- Handle vision feature --------\n",
        "        if self.split == 'test' and self.task_type in ['middle_audio', 'lower_audio']:\n",
        "            vision_feat = sample['generated_vision'].squeeze(1).permute(1, 0).float()\n",
        "        else:\n",
        "            vision_feat = sample['original_vision'].squeeze(1).float()\n",
        "\n",
        "        vision_feat = pad_tensor_to_shape(vision_feat, (512,5))\n",
        "\n",
        "        # -------- Assemble inputs --------\n",
        "        input_feats = {}\n",
        "        if self.task_type in ['upper', 'middle_audio', 'middle_vision']:\n",
        "            input_feats = {'audio': audio_feat, 'vision': vision_feat, 'text': text_feat}\n",
        "        elif self.task_type == 'lower_audio':\n",
        "            input_feats = {'vision': vision_feat, 'text': text_feat}\n",
        "        elif self.task_type == 'lower_vision':\n",
        "            input_feats = {'audio': audio_feat, 'text': text_feat}\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown task type {self.task_type}\")\n",
        "\n",
        "        # -------- Assemble target --------\n",
        "        traits = torch.tensor([\n",
        "            sample['agreeableness'],\n",
        "            sample['openness'],\n",
        "            sample['neuroticism'],\n",
        "            sample['extraversion'],\n",
        "            sample['conscientiousness']\n",
        "        ], dtype=torch.float32)\n",
        "\n",
        "        return input_feats, traits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b61eda98-11bb-47a5-8fc1-b955c7b52730",
      "metadata": {
        "id": "b61eda98-11bb-47a5-8fc1-b955c7b52730"
      },
      "outputs": [],
      "source": [
        "class ModalityLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch, feature_dim, time_steps)\n",
        "        \"\"\"\n",
        "        x = x.permute(0, 2, 1)  # (batch, time_steps, feature_dim)\n",
        "        out, (h_n, c_n) = self.lstm(x)\n",
        "        return h_n[-1]  # Take last hidden state\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, input_dim=768, output_dim=256):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a9201b9-1c52-4b56-8306-4dd053cbba79",
      "metadata": {
        "id": "6a9201b9-1c52-4b56-8306-4dd053cbba79"
      },
      "outputs": [],
      "source": [
        "class EarlyFusionRegressor(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dim=256, output_dim=5):\n",
        "        super().__init__()\n",
        "        self.input_dims = input_dims\n",
        "        total_input_dim = sum(input_dims.values())\n",
        "        self.fc1 = nn.Linear(total_input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # inputs: dict of {'audio': tensor, 'vision': tensor, 'text': tensor}\n",
        "        x = torch.cat(list(inputs.values()), dim=-1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class LateFusionRegressor(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dim=256, output_dim=5):\n",
        "        super().__init__()\n",
        "        self.modalities = nn.ModuleDict({\n",
        "            k: nn.Sequential(\n",
        "                nn.Linear(dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, output_dim)\n",
        "            )\n",
        "            for k, dim in input_dims.items()\n",
        "        })\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        preds = []\n",
        "        for k, v in inputs.items():\n",
        "            preds.append(self.modalities[k](v))\n",
        "        # Mean the outputs\n",
        "        preds = torch.stack(preds, dim=0).mean(dim=0)\n",
        "        return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33d336f4-ebcd-4465-90df-64316f5a5f86",
      "metadata": {
        "id": "33d336f4-ebcd-4465-90df-64316f5a5f86"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from scipy.stats import pearsonr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76d05d55-78de-46f5-9b09-25ac7d50a8dd",
      "metadata": {
        "id": "76d05d55-78de-46f5-9b09-25ac7d50a8dd"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, optimizer, loss_fn=nn.MSELoss(), device='cuda'):\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.device = device\n",
        "\n",
        "    def train(self, train_loader, val_loader, epochs=50, patience=5):\n",
        "        best_val_loss = float('inf')\n",
        "        best_model = None\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            total_train_loss = 0\n",
        "            for batch_x, batch_y in train_loader:\n",
        "                batch_x = {k: v.to(self.device) for k, v in batch_x.items()}\n",
        "                batch_y = batch_y.to(self.device)\n",
        "\n",
        "                preds = self.model(batch_x)\n",
        "                loss = self.loss_fn(preds, batch_y)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "            val_loss = self.evaluate_loss(val_loader)\n",
        "            print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model = self.model.state_dict()\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered!\")\n",
        "                break\n",
        "\n",
        "        self.model.load_state_dict(best_model)\n",
        "\n",
        "    def evaluate_loss(self, loader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in loader:\n",
        "                batch_x = {k: v.to(self.device) for k, v in batch_x.items()}\n",
        "                batch_y = batch_y.to(self.device)\n",
        "\n",
        "                preds = self.model(batch_x)\n",
        "                loss = self.loss_fn(preds, batch_y)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18aa0f72-c307-454a-912c-30ee20e4848a",
      "metadata": {
        "id": "18aa0f72-c307-454a-912c-30ee20e4848a"
      },
      "outputs": [],
      "source": [
        "class Evaluator:\n",
        "    @staticmethod\n",
        "    def evaluate(model, loader, device='cuda'):\n",
        "        model.eval()\n",
        "        preds_list, labels_list = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in loader:\n",
        "                batch_x = {k: v.to(device) for k, v in batch_x.items()}\n",
        "                batch_y = batch_y.to(device)\n",
        "\n",
        "                preds = model(batch_x)\n",
        "                preds_list.append(preds.cpu())\n",
        "                labels_list.append(batch_y.cpu())\n",
        "\n",
        "        preds = torch.cat(preds_list, dim=0).numpy()\n",
        "        labels = torch.cat(labels_list, dim=0).numpy()\n",
        "\n",
        "        return Evaluator.compute_metrics(preds, labels)\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_metrics(preds, labels):\n",
        "        results = {}\n",
        "        num_traits = preds.shape[1]\n",
        "\n",
        "        for i in range(num_traits):\n",
        "            p = preds[:, i]\n",
        "            l = labels[:, i]\n",
        "\n",
        "            mae = np.mean(np.abs(p - l))\n",
        "            acc = 1 - mae\n",
        "            mse = mean_squared_error(l, p)\n",
        "            r2 = r2_score(l, p)\n",
        "            pcc, _ = pearsonr(l, p)\n",
        "            mean_p = np.mean(p)\n",
        "            mean_l = np.mean(l)\n",
        "            var_p = np.var(p)\n",
        "            var_l = np.var(l)\n",
        "            ccc = (2 * pcc * np.sqrt(var_p) * np.sqrt(var_l)) / (var_p + var_l + (mean_p - mean_l) ** 2)\n",
        "\n",
        "            results[f'trait_{i+1}'] = {\n",
        "                'MAE': mae,\n",
        "                'ACC': acc,\n",
        "                'MSE': mse,\n",
        "                'R2': r2,\n",
        "                'PCC': pcc,\n",
        "                'CCC': ccc\n",
        "            }\n",
        "\n",
        "        # Aggregate metrics\n",
        "        avg_metrics = {metric: np.mean([results[f'trait_{i+1}'][metric] for i in range(num_traits)]) for metric in ['MAE', 'ACC', 'MSE', 'R2', 'PCC', 'CCC']}\n",
        "        results['average'] = avg_metrics\n",
        "\n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1403c350-9a70-4188-8b53-bdad80baba37",
      "metadata": {
        "id": "1403c350-9a70-4188-8b53-bdad80baba37"
      },
      "outputs": [],
      "source": [
        "class ExperimentRunner:\n",
        "    def __init__(self, train_data, val_data, test_data, batch_size=64, device='cuda'):\n",
        "        self.train_data = train_data\n",
        "        self.val_data = val_data\n",
        "        self.test_data = test_data\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "\n",
        "    def run(self, task_type, fusion_type):\n",
        "        print(f\"\\n=== Running Task: {task_type.upper()}, Fusion: {fusion_type.upper()} ===\")\n",
        "\n",
        "        train_dataset = PersonalityDataset(self.train_data, split='train', task_type=task_type)\n",
        "        val_dataset = PersonalityDataset(self.val_data, split='val', task_type=task_type)\n",
        "        test_dataset = PersonalityDataset(self.test_data, split='test', task_type=task_type)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "        input_dims = self.get_input_dims(train_dataset)\n",
        "\n",
        "        audio_lstm = ModalityLSTM(1024, hidden_dim=256) if 'audio' in input_dims else None\n",
        "        vision_lstm = ModalityLSTM(512, hidden_dim=256) if 'vision' in input_dims else None\n",
        "        text_encoder = TextEncoder(768, output_dim=256)\n",
        "\n",
        "        if fusion_type == 'early':\n",
        "            model = EarlyFusionRegressor(input_dims={k: 256 for k in input_dims})\n",
        "        else:\n",
        "            model = LateFusionRegressor(input_dims={k: 256 for k in input_dims})\n",
        "\n",
        "        full_model = FullModel(audio_lstm, vision_lstm, text_encoder, model, device=self.device)\n",
        "        optimizer = torch.optim.Adam(full_model.parameters(), lr=1e-3)\n",
        "\n",
        "        trainer = Trainer(full_model, optimizer, loss_fn=nn.MSELoss(), device=self.device)\n",
        "        trainer.train(train_loader, val_loader, epochs=50, patience=5)\n",
        "\n",
        "        evaluator = Evaluator()\n",
        "        results = evaluator.evaluate(full_model, test_loader, device=self.device)\n",
        "\n",
        "        print(\"Results:\")\n",
        "        for trait, metrics in results.items():\n",
        "            print(f\"{trait}: {metrics}\")\n",
        "\n",
        "    def get_input_dims(self, dataset):\n",
        "        sample, _ = dataset[0]\n",
        "        return {k: v.shape[-1] for k, v in sample.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d8febe3-1fc6-494c-b389-994abe77d316",
      "metadata": {
        "id": "2d8febe3-1fc6-494c-b389-994abe77d316"
      },
      "outputs": [],
      "source": [
        "class FullModel(nn.Module):\n",
        "    def __init__(self, audio_lstm, vision_lstm, text_encoder, fusion_model, device='cuda'):\n",
        "        super().__init__()\n",
        "        self.audio_lstm = audio_lstm.to(device) if audio_lstm else None\n",
        "        self.vision_lstm = vision_lstm.to(device) if vision_lstm else None\n",
        "        self.text_encoder = text_encoder.to(device)\n",
        "        self.fusion_model = fusion_model.to(device)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        feats = {}\n",
        "        if 'audio' in inputs and self.audio_lstm:\n",
        "            feats['audio'] = self.audio_lstm(inputs['audio'])\n",
        "        if 'vision' in inputs and self.vision_lstm:\n",
        "            feats['vision'] = self.vision_lstm(inputs['vision'])\n",
        "        if 'text' in inputs:\n",
        "            feats['text'] = self.text_encoder(inputs['text'])\n",
        "\n",
        "        return self.fusion_model(feats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb071a16-31d5-4d16-b4f9-71daa9aa29d8",
      "metadata": {
        "id": "cb071a16-31d5-4d16-b4f9-71daa9aa29d8"
      },
      "outputs": [],
      "source": [
        "TASKS = ['upper', 'middle_audio', 'middle_vision', 'lower_audio', 'lower_vision']\n",
        "FUSIONS = ['early', 'late']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00fbde05-e73f-46bd-88fa-bf2a7bbe6a58",
      "metadata": {
        "id": "00fbde05-e73f-46bd-88fa-bf2a7bbe6a58"
      },
      "outputs": [],
      "source": [
        "runner = ExperimentRunner(train_data, val_data, test_data, batch_size=16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3a8feaf0-4928-4c6b-8f3b-cbe972f06f5d",
      "metadata": {
        "id": "3a8feaf0-4928-4c6b-8f3b-cbe972f06f5d"
      },
      "outputs": [],
      "source": [
        "runner.run('upper', 'early')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1245e20-8c17-4784-a7b5-529e913459a0",
      "metadata": {
        "id": "f1245e20-8c17-4784-a7b5-529e913459a0"
      },
      "outputs": [],
      "source": [
        "# OCEAN Trait Names\n",
        "OCEAN_TRAITS = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Neuroticism']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0344755a-ed0f-4f96-bad5-fb2ee4406dc8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0344755a-ed0f-4f96-bad5-fb2ee4406dc8",
        "outputId": "6751e9db-bb86-4887-caa4-920019bd6dc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved results to results/results_upper_late.json ✅\n"
          ]
        }
      ],
      "source": [
        "# Upper Bound - Late Fusion\n",
        "task = 'upper'\n",
        "fusion = 'late'\n",
        "results = runner.run(task, fusion)\n",
        "\n",
        "import json, os\n",
        "os.makedirs('results', exist_ok=True)\n",
        "with open(f'results/results_{task}_{fusion}.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "print(f\"Saved results to results/results_{task}_{fusion}.json ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "468657ab-b0a5-475d-aef5-481bd0e4a6d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "468657ab-b0a5-475d-aef5-481bd0e4a6d2",
        "outputId": "8a240f1f-30b6-43a9-854b-3c70f8326b3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved results to results/results_upper_early.json ✅\n"
          ]
        }
      ],
      "source": [
        "# Upper Bound - Early Fusion\n",
        "task = 'upper'\n",
        "fusion = 'early'\n",
        "results = runner.run(task, fusion)\n",
        "\n",
        "import json, os\n",
        "os.makedirs('results', exist_ok=True)\n",
        "with open(f'results/results_{task}_{fusion}.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "print(f\"Saved results to results/results_{task}_{fusion}.json ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "47c15102-1d63-4eff-90cc-893992b16600",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47c15102-1d63-4eff-90cc-893992b16600",
        "outputId": "8f225fcc-5347-4856-eb87-c84178a54f50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved results to results/results_middle_audio_early.json ✅\n"
          ]
        }
      ],
      "source": [
        "# Middle Audio - Early Fusion\n",
        "task = 'middle_audio'\n",
        "fusion = 'early'\n",
        "results = runner.run(task, fusion)\n",
        "\n",
        "import json, os\n",
        "os.makedirs('results', exist_ok=True)\n",
        "with open(f'results/results_{task}_{fusion}.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "print(f\"Saved results to results/results_{task}_{fusion}.json ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7861db87-13c3-46aa-8d30-79d773f4cd3d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7861db87-13c3-46aa-8d30-79d773f4cd3d",
        "outputId": "6c2acb0f-3c86-440f-f7e8-2142571b99d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved results to results/results_middle_audio_late.json ✅\n"
          ]
        }
      ],
      "source": [
        "# Middle Audio - Late Fusion\n",
        "task = 'middle_audio'\n",
        "fusion = 'late'\n",
        "results = runner.run(task, fusion)\n",
        "\n",
        "import json, os\n",
        "os.makedirs('results', exist_ok=True)\n",
        "with open(f'results/results_{task}_{fusion}.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "print(f\"Saved results to results/results_{task}_{fusion}.json ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d2b8b3a7-e5d8-48c0-b0b4-a453cf01d5f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2b8b3a7-e5d8-48c0-b0b4-a453cf01d5f5",
        "outputId": "4bffd58f-8ab7-4c63-d741-1caae6dd141c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved results to results/results_middle_vision_early.json ✅\n"
          ]
        }
      ],
      "source": [
        "# Middle Vision - Early Fusion\n",
        "task = 'middle_vision'\n",
        "fusion = 'early'\n",
        "results = runner.run(task, fusion)\n",
        "\n",
        "import json, os\n",
        "os.makedirs('results', exist_ok=True)\n",
        "with open(f'results/results_{task}_{fusion}.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "print(f\"Saved results to results/results_{task}_{fusion}.json ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ad105f9b-c677-4023-845d-24013c9d58d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad105f9b-c677-4023-845d-24013c9d58d8",
        "outputId": "d7fce5f6-d522-4ce4-a18e-74bf96edf9e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved results to results/results_middle_vision_late.json ✅\n"
          ]
        }
      ],
      "source": [
        "# Middle Vision - Late Fusion\n",
        "task = 'middle_vision'\n",
        "fusion = 'late'\n",
        "results = runner.run(task, fusion)\n",
        "\n",
        "import json, os\n",
        "os.makedirs('results', exist_ok=True)\n",
        "with open(f'results/results_{task}_{fusion}.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "print(f\"Saved results to results/results_{task}_{fusion}.json ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "cc1ff6f4-0a88-4819-bc29-fdec20367ff6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc1ff6f4-0a88-4819-bc29-fdec20367ff6",
        "outputId": "beb31e02-7138-4b8c-82b1-b28dec94c2dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved results to results/results_lower_audio_early.json ✅\n"
          ]
        }
      ],
      "source": [
        "# Lower Audio - Early Fusion\n",
        "task = 'lower_audio'\n",
        "fusion = 'early'\n",
        "results = runner.run(task, fusion)\n",
        "\n",
        "import json, os\n",
        "os.makedirs('results', exist_ok=True)\n",
        "with open(f'results/results_{task}_{fusion}.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "print(f\"Saved results to results/results_{task}_{fusion}.json ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "459aa763-d884-48ec-a984-96ef4d2de6fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "459aa763-d884-48ec-a984-96ef4d2de6fa",
        "outputId": "db456523-2cbb-4f10-ae28-c8387360a906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved results to results/results_lower_audio_late.json ✅\n"
          ]
        }
      ],
      "source": [
        "# Lower Audio - Late Fusion\n",
        "task = 'lower_audio'\n",
        "fusion = 'late'\n",
        "results = runner.run(task, fusion)\n",
        "\n",
        "import json, os\n",
        "os.makedirs('results', exist_ok=True)\n",
        "with open(f'results/results_{task}_{fusion}.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "print(f\"Saved results to results/results_{task}_{fusion}.json ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0d7f6255-c6a1-4bf6-92df-dad3bd103f2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d7f6255-c6a1-4bf6-92df-dad3bd103f2d",
        "outputId": "e4c4f40c-2741-44aa-82e6-b6aff51f3da2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved results to results/results_lower_vision_early.json ✅\n"
          ]
        }
      ],
      "source": [
        "# Lower Vision - Early Fusion\n",
        "task = 'lower_vision'\n",
        "fusion = 'early'\n",
        "results = runner.run(task, fusion)\n",
        "\n",
        "import json, os\n",
        "os.makedirs('results', exist_ok=True)\n",
        "with open(f'results/results_{task}_{fusion}.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "print(f\"Saved results to results/results_{task}_{fusion}.json ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3d0dd877-c34a-4f3c-a9d5-c0cfe64cf495",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d0dd877-c34a-4f3c-a9d5-c0cfe64cf495",
        "outputId": "b6b5f747-300f-4938-b0ac-fd65af28d35e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved results to results/results_lower_vision_late.json ✅\n"
          ]
        }
      ],
      "source": [
        "# Lower Vision - Late Fusion\n",
        "task = 'lower_vision'\n",
        "fusion = 'late'\n",
        "results = runner.run(task, fusion)\n",
        "\n",
        "import json, os\n",
        "os.makedirs('results', exist_ok=True)\n",
        "with open(f'results/results_{task}_{fusion}.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "print(f\"Saved results to results/results_{task}_{fusion}.json ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb2a1b33-ed73-4999-bd2b-0cbaf9f2b129",
      "metadata": {
        "id": "eb2a1b33-ed73-4999-bd2b-0cbaf9f2b129"
      },
      "outputs": [],
      "source": [
        "class ConditionalDataset(Dataset):\n",
        "    def __init__(self, data, target='visual'):\n",
        "        self.data = data\n",
        "        self.keys = list(data.keys())\n",
        "        self.target = target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.keys)\n",
        "\n",
        "    def pad_segments(self, tensor, target_segments=5):\n",
        "        current_segments = tensor.size(0)\n",
        "        if current_segments < target_segments:\n",
        "            pad_size = target_segments - current_segments\n",
        "            padding = torch.zeros(pad_size, tensor.size(1))\n",
        "            tensor = torch.cat([tensor, padding], dim=0)\n",
        "        return tensor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        key = self.keys[idx]\n",
        "        sample = self.data[key]\n",
        "\n",
        "        text_feat = sample['original_text'].float()  # (768,)\n",
        "\n",
        "        if self.target == 'visual':\n",
        "            audio_feat = sample['original_audio']\n",
        "\n",
        "            if audio_feat.dim() == 3:\n",
        "                audio_feat = audio_feat.mean(dim=1)  # (1024, 5)\n",
        "            elif audio_feat.dim() == 2:\n",
        "                pass\n",
        "            elif audio_feat.dim() == 1:\n",
        "                audio_feat = audio_feat.unsqueeze(1)  # (1024, 1)\n",
        "            else:\n",
        "                return None  # bad data\n",
        "\n",
        "            if audio_feat.shape[0] != 1024 or audio_feat.shape[-1] == 0:\n",
        "                return None\n",
        "\n",
        "            cond_feat = audio_feat.permute(1, 0)  # (segments, 1024)\n",
        "            cond_feat = self.pad_segments(cond_feat, target_segments=5)\n",
        "\n",
        "        elif self.target == 'audio':\n",
        "            vision_feat = sample['original_vision']\n",
        "            if vision_feat.dim() == 3:\n",
        "                vision_feat = vision_feat.squeeze(1).float()  # (512, T)\n",
        "            elif vision_feat.dim() == 2:\n",
        "                vision_feat = vision_feat.float()\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "            if vision_feat.shape[0] != 512 or vision_feat.shape[-1] == 0:\n",
        "                return None\n",
        "\n",
        "            cond_feat = vision_feat.permute(1, 0)  # (segments, 512)\n",
        "            cond_feat = self.pad_segments(cond_feat, target_segments=5)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"target must be 'visual' or 'audio'\")\n",
        "\n",
        "        return key, cond_feat, text_feat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a5f281a-1caf-4342-a2dc-3d7b882a7973",
      "metadata": {
        "id": "1a5f281a-1caf-4342-a2dc-3d7b882a7973"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SegmentConditionalVAE(nn.Module):\n",
        "    def __init__(self, text_dim=768, audio_dim=1024, visual_dim=512, latent_dim=128, target='visual'):\n",
        "        super().__init__()\n",
        "\n",
        "        # Decide which modality is missing (target), and which is available (conditioning)\n",
        "        self.target = target\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        if target == 'visual':\n",
        "            self.input_modality_dim = audio_dim\n",
        "            self.output_dim = visual_dim\n",
        "        elif target == 'audio':\n",
        "            self.input_modality_dim = visual_dim\n",
        "            self.output_dim = audio_dim\n",
        "        else:\n",
        "            raise ValueError(\"target must be 'visual' or 'audio'\")\n",
        "\n",
        "        self.segment_dim = 5  # number of segments (fixed for now)\n",
        "\n",
        "        # ------ ENCODER -------\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(self.input_modality_dim + text_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(256, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
        "\n",
        "        # ------ DECODER -------\n",
        "        self.decoder_input = nn.Linear(latent_dim + self.input_modality_dim + text_dim, 256)\n",
        "        self.decoder = nn.Sequential(\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, self.output_dim)\n",
        "    )\n",
        "\n",
        "    def encode(self, condition_segments, text):\n",
        "        \"\"\"\n",
        "        Encode the conditioning modality + text into μ and logσ²\n",
        "        \"\"\"\n",
        "        B, T, _ = condition_segments.shape\n",
        "        text = text.unsqueeze(1).expand(-1, T, -1)  # (B, 5, 768)\n",
        "        concat = torch.cat([condition_segments, text], dim=-1)  # (B, 5, cond+768)\n",
        "        hidden = self.encoder(concat)                # (B, 5, 256)\n",
        "        mu = self.fc_mu(hidden)                      # (B, 5, latent_dim)\n",
        "        logvar = self.fc_logvar(hidden)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        \"\"\"\n",
        "        Sample z from N(μ, σ²) using the reparameterization trick\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z, condition_segments, text):\n",
        "        \"\"\"\n",
        "        Decode the latent z with conditioning inputs into reconstructed output\n",
        "        \"\"\"\n",
        "        B, T, _ = z.shape\n",
        "        text = text.unsqueeze(1).expand(-1, T, -1)  # (B, 5, 768)\n",
        "        dec_input = torch.cat([z, condition_segments, text], dim=-1)  # (B, 5, latent+cond+text)\n",
        "        dec_input = self.decoder_input(dec_input)  # (B, 5, 256)\n",
        "        out = self.decoder(dec_input)              # (B, 5, output_dim)\n",
        "        return out\n",
        "\n",
        "    def forward(self, condition_segments, text, use_mean=False):\n",
        "        \"\"\"\n",
        "        Forward pass for training or inference.\n",
        "\n",
        "        Args:\n",
        "            condition_segments: (B, T, input_modality_dim)\n",
        "            text: (B, 768)\n",
        "            use_mean (bool): If True, use z = μ. Otherwise, sample using z = μ + σ * ε\n",
        "\n",
        "        Returns:\n",
        "            out: reconstructed modality\n",
        "            mu, logvar: latent distribution parameters\n",
        "        \"\"\"\n",
        "        mu, logvar = self.encode(condition_segments, text)\n",
        "\n",
        "        # Safety: warn if use_mean=True during training (not recommended)\n",
        "        if self.training and use_mean:\n",
        "            print(\"[Warning] use_mean=True during training — model will behave deterministically. \"\n",
        "                  \"You probably want to set use_mean=False for stochastic training.\")\n",
        "\n",
        "        if use_mean:\n",
        "            z = mu\n",
        "        else:\n",
        "            z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        out = self.decode(z, condition_segments, text)\n",
        "        return out, mu, logvar\n",
        "\n",
        "\n",
        "    def sample_from_prior(self, condition_segments, text):\n",
        "        \"\"\"\n",
        "        Pure generative inference: sample z ~ N(0, I), no encoder needed.\n",
        "        \"\"\"\n",
        "        B, T, _ = condition_segments.shape\n",
        "        z = torch.randn((B, T, self.latent_dim), device=condition_segments.device)\n",
        "        out = self.decode(z, condition_segments, text)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac2bb971-a3ed-4de0-8607-630c582e6fd3",
      "metadata": {
        "id": "ac2bb971-a3ed-4de0-8607-630c582e6fd3",
        "outputId": "b1e08edb-a2cc-444a-d68a-ac0f9d114395"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SegmentConditionalVAE(\n",
              "  (encoder): Sequential(\n",
              "    (0): Linear(in_features=1792, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (3): ReLU()\n",
              "  )\n",
              "  (fc_mu): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc_logvar): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (decoder_input): Linear(in_features=1920, out_features=256, bias=True)\n",
              "  (decoder): Sequential(\n",
              "    (0): ReLU()\n",
              "    (1): Linear(in_features=256, out_features=512, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (6): ReLU()\n",
              "    (7): Linear(in_features=512, out_features=512, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Assuming you have your CVAE model class defined as SegmentConditionalVAE\n",
        "model = SegmentConditionalVAE(target='visual')  # or 'audio', depending on what you're reconstructing\n",
        "checkpoint = torch.load('cvae_visual_beta_anneal.pt', map_location='cuda')\n",
        "model.load_state_dict(checkpoint)\n",
        "model.eval()\n",
        "model.cuda()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da998960-e67f-4625-a6be-ca10b09bd24f",
      "metadata": {
        "id": "da998960-e67f-4625-a6be-ca10b09bd24f"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def reconstruct(model, loader, store_dict, feature_name):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for keys, cond_feats, text_feats in tqdm(loader):\n",
        "            cond_feats = cond_feats.cuda()\n",
        "            text_feats = text_feats.cuda()\n",
        "\n",
        "            recon_x, mu, logvar = model(cond_feats, text_feats, use_mean=True)\n",
        "            recon_x = recon_x.cpu()\n",
        "\n",
        "            for i, key in enumerate(keys):\n",
        "                if key in store_dict:\n",
        "                    store_dict[key][feature_name] = recon_x[i]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39b8feda-94f3-4b01-9a89-d4d5fa543c11",
      "metadata": {
        "id": "39b8feda-94f3-4b01-9a89-d4d5fa543c11",
        "outputId": "bb28b803-5fd6-4877-b9b8-722c5878ebe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tqdm\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm\n",
            "Successfully installed tqdm-4.67.1\n"
          ]
        }
      ],
      "source": [
        "# import sys\n",
        "# !{sys.executable} -m pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "016647e0-0569-4e65-b480-311be57e4f87",
      "metadata": {
        "id": "016647e0-0569-4e65-b480-311be57e4f87"
      },
      "outputs": [],
      "source": [
        "def safe_collate(batch):\n",
        "    batch = [b for b in batch if b is not None]  # Remove bad samples\n",
        "\n",
        "    keys, cond_feats, text_feats = zip(*batch)\n",
        "\n",
        "    keys = list(keys)\n",
        "    cond_feats = torch.stack(cond_feats, dim=0)\n",
        "    text_feats = torch.stack(text_feats, dim=0)\n",
        "\n",
        "    return keys, cond_feats, text_feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cb7dabb-275a-465f-8e2d-5a8d5054006c",
      "metadata": {
        "id": "1cb7dabb-275a-465f-8e2d-5a8d5054006c"
      },
      "outputs": [],
      "source": [
        "# # ---- Choose your target ----\n",
        "# TARGET = 'visual'   # 'visual' or 'audio'\n",
        "\n",
        "# # ---- Load the right CVAE model ----\n",
        "# model = SegmentConditionalVAE(target=TARGET)\n",
        "# model.load_state_dict(torch.load('cvae_visual_beta_anneal.pt', map_location='cuda'))\n",
        "# model.cuda()\n",
        "\n",
        "# # ---- Prepare datasets/loaders ----\n",
        "# train_dataset = ConditionalDataset(train_data, target=TARGET)\n",
        "# val_dataset = ConditionalDataset(val_data, target=TARGET)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False, collate_fn=safe_collate)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=safe_collate)\n",
        "\n",
        "# # ---- Create dicts to store reconstructed data ----\n",
        "# reconstructed_train = {}\n",
        "# reconstructed_val = {}\n",
        "\n",
        "# # ---- Run Reconstruction ----\n",
        "# feature_name = 'reconstructed_' + TARGET  # 'reconstructed_visual' or 'reconstructed_audio'\n",
        "\n",
        "# reconstruct(model, train_loader, train_data, reconstructed_train, feature_name)\n",
        "# reconstruct(model, val_loader, val_data, reconstructed_val, feature_name)\n",
        "\n",
        "# # ---- Save the new pickles ----\n",
        "# with open(f'reconstructed_cvae_train_{TARGET}.pkl', 'wb') as f:\n",
        "#     pickle.dump(reconstructed_train, f)\n",
        "\n",
        "# with open(f'reconstructed_cvae_val_{TARGET}.pkl', 'wb') as f:\n",
        "#     pickle.dump(reconstructed_val, f)\n",
        "\n",
        "# print(f\"✅ Done reconstructing and saving for {TARGET}!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a5a7469-c9cf-4f7f-b03a-187d69b8d7e7",
      "metadata": {
        "id": "2a5a7469-c9cf-4f7f-b03a-187d69b8d7e7",
        "outputId": "8692a8a4-5ff3-4520-9b51-ecf31eb0adc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔵 Now reconstructing visual...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 94/94 [00:18<00:00,  5.04it/s]\n",
            "100%|██████████| 32/32 [00:06<00:00,  4.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔵 Now reconstructing audio...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 506.55it/s]\n",
            "100%|██████████| 32/32 [00:00<00:00, 499.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All modalities reconstructed!\n",
            "✅ Final reconstructed pickles saved!\n"
          ]
        }
      ],
      "source": [
        "# Make deep copies so we can safely add new fields\n",
        "reconstructed_train = {k: v.copy() for k, v in train_data.items()}\n",
        "reconstructed_val = {k: v.copy() for k, v in val_data.items()}\n",
        "\n",
        "# ---- Reconstruct for both targets ----\n",
        "for TARGET, MODEL_PATH in [('visual', 'cvae_visual_beta_anneal.pt'), ('audio', 'cvae_audio_beta_anneal.pt')]:\n",
        "\n",
        "    print(f\"🔵 Now reconstructing {TARGET}...\")\n",
        "\n",
        "    # Load the right model\n",
        "    model = SegmentConditionalVAE(target=TARGET)\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location='cuda'))\n",
        "    model.cuda()\n",
        "\n",
        "    # Prepare datasets/loaders\n",
        "    train_dataset = ConditionalDataset(train_data, target=TARGET)\n",
        "    val_dataset = ConditionalDataset(val_data, target=TARGET)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False, collate_fn=safe_collate)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=safe_collate)\n",
        "\n",
        "    # Run reconstruction\n",
        "    feature_name = 'reconstructed_' + TARGET  # 'reconstructed_visual' or 'reconstructed_audio'\n",
        "\n",
        "    reconstruct(model, train_loader, reconstructed_train, feature_name)\n",
        "    reconstruct(model, val_loader, reconstructed_val, feature_name)\n",
        "\n",
        "print(\"✅ All modalities reconstructed!\")\n",
        "\n",
        "# ---- Save the merged pickles ----\n",
        "with open('reconstructed_cvae_train.pkl', 'wb') as f:\n",
        "    pickle.dump(reconstructed_train, f)\n",
        "\n",
        "with open('reconstructed_cvae_val.pkl', 'wb') as f:\n",
        "    pickle.dump(reconstructed_val, f)\n",
        "\n",
        "print(\"✅ Final reconstructed pickles saved!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f6535a5-eeb5-4f42-b35d-00295d7d9732",
      "metadata": {
        "id": "0f6535a5-eeb5-4f42-b35d-00295d7d9732",
        "outputId": "ded060c2-8744-4ec5-e2d9-9ac6acf14570"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'reconstructed_val' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mreconstructed_val\u001b[49m[\u001b[38;5;28mlist\u001b[39m(reconstructed_val.keys())[\u001b[32m0\u001b[39m]].keys()\n",
            "\u001b[31mNameError\u001b[39m: name 'reconstructed_val' is not defined"
          ]
        }
      ],
      "source": [
        "reconstructed_val[list(reconstructed_val.keys())[0]].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1629d85-7375-407b-9ae0-315efa857c8c",
      "metadata": {
        "id": "f1629d85-7375-407b-9ae0-315efa857c8c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "def pad_tensor_to_shape(x, target_shape):\n",
        "    \"\"\"Pads a tensor with zeros to match the target shape.\"\"\"\n",
        "    current_shape = x.shape\n",
        "    pad = []\n",
        "    for c, t in zip(reversed(current_shape), reversed(target_shape)):\n",
        "        pad.extend([0, max(t - c, 0)])\n",
        "    return F.pad(x, pad)\n",
        "\n",
        "class PersonalityDataset(Dataset):\n",
        "    def __init__(self, data_dict, split, task_type):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dict: dictionary from loaded pickle\n",
        "            split: 'train', 'val', 'test'\n",
        "            task_type: 'upper', 'middle_audio', 'middle_vision', 'lower_audio', 'lower_vision'\n",
        "        \"\"\"\n",
        "        self.data = data_dict\n",
        "        self.split = split\n",
        "        self.task_type = task_type\n",
        "        self.keys = list(data_dict.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.keys)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        vid = self.keys[idx]\n",
        "        sample = self.data[vid]\n",
        "\n",
        "        text_feat = sample['original_text'].float()\n",
        "\n",
        "        # === Handle AUDIO ===\n",
        "        if self.task_type in ['middle_vision', 'lower_vision']:\n",
        "            if self.split == 'test':\n",
        "                audio = sample['generated_audio'].squeeze(1).permute(1, 0).float()\n",
        "            else:\n",
        "                if 'reconstructed_audio' in sample:\n",
        "                    audio = sample['reconstructed_audio'].permute(1, 0).float()  # (5,1024) -> (1024,5)\n",
        "                else:\n",
        "                    audio = sample['original_audio'].float()\n",
        "\n",
        "        else:\n",
        "            audio = sample['original_audio'].float()\n",
        "\n",
        "        if audio.dim() == 3:\n",
        "            audio_feat = audio.mean(dim=1)\n",
        "        elif audio.dim() == 2:\n",
        "            audio_feat = audio\n",
        "        elif audio.dim() == 1:\n",
        "            audio_feat = audio.unsqueeze(1).repeat(1, 5)\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected audio feature shape {audio.shape} for video {vid}\")\n",
        "\n",
        "        audio_feat = pad_tensor_to_shape(audio_feat, (1024, 5))\n",
        "\n",
        "\n",
        "        # === Handle VISION ===\n",
        "        if self.task_type in ['middle_audio', 'lower_audio']:\n",
        "            if self.split == 'test':\n",
        "                vision_feat = sample['generated_vision'].squeeze(1).permute(1, 0).float()\n",
        "            else:\n",
        "                if 'reconstructed_visual' in sample:\n",
        "                    vision_feat = sample['reconstructed_visual'].permute(1, 0).float()  # (5,512) -> (512,5)\n",
        "                else:\n",
        "                    vision_feat = sample['original_vision'].squeeze(1).float()\n",
        "\n",
        "        else:\n",
        "            vision_feat = sample['original_vision'].squeeze(1).float()\n",
        "\n",
        "        vision_feat = pad_tensor_to_shape(vision_feat, (512, 5))\n",
        "\n",
        "\n",
        "        # === Assemble Input Features ===\n",
        "        input_feats = {}\n",
        "        if self.task_type in ['upper', 'middle_audio', 'middle_vision']:\n",
        "            input_feats = {'audio': audio_feat, 'vision': vision_feat, 'text': text_feat}\n",
        "        elif self.task_type == 'lower_audio':\n",
        "            input_feats = {'vision': vision_feat, 'text': text_feat}\n",
        "        elif self.task_type == 'lower_vision':\n",
        "            input_feats = {'audio': audio_feat, 'text': text_feat}\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown task type {self.task_type}\")\n",
        "\n",
        "        # === Target Traits ===\n",
        "        traits = torch.tensor([\n",
        "            sample['agreeableness'],\n",
        "            sample['openness'],\n",
        "            sample['neuroticism'],\n",
        "            sample['extraversion'],\n",
        "            sample['conscientiousness']\n",
        "        ], dtype=torch.float32)\n",
        "\n",
        "        return input_feats, traits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be4ebacf-27a0-483d-adcb-d5b24fbc701d",
      "metadata": {
        "id": "be4ebacf-27a0-483d-adcb-d5b24fbc701d"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def make_json_serializable(obj):\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: make_json_serializable(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [make_json_serializable(v) for v in obj]\n",
        "    elif hasattr(obj, \"item\"):  # for np.float32, np.float64, etc.\n",
        "        return obj.item()\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "class ExperimentRunner:\n",
        "    def __init__(self, train_data, val_data, test_data, batch_size=64, device='cuda'):\n",
        "        self.train_data = train_data\n",
        "        self.val_data = val_data\n",
        "        self.test_data = test_data\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "\n",
        "    def run(self, task_type, fusion_type, result_prefix=''):\n",
        "        print(f\"\\n=== Running Task: {task_type.upper()}, Fusion: {fusion_type.upper()} ===\")\n",
        "\n",
        "        train_dataset = PersonalityDataset(self.train_data, split='train', task_type=task_type)\n",
        "        val_dataset = PersonalityDataset(self.val_data, split='val', task_type=task_type)\n",
        "        test_dataset = PersonalityDataset(self.test_data, split='test', task_type=task_type)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "        input_dims = self.get_input_dims(train_dataset)\n",
        "\n",
        "        audio_lstm = ModalityLSTM(1024, hidden_dim=256) if 'audio' in input_dims else None\n",
        "        vision_lstm = ModalityLSTM(512, hidden_dim=256) if 'vision' in input_dims else None\n",
        "        text_encoder = TextEncoder(768, output_dim=256)\n",
        "\n",
        "        if fusion_type == 'early':\n",
        "            model = EarlyFusionRegressor(input_dims={k: 256 for k in input_dims})\n",
        "        else:\n",
        "            model = LateFusionRegressor(input_dims={k: 256 for k in input_dims})\n",
        "\n",
        "        full_model = FullModel(audio_lstm, vision_lstm, text_encoder, model, device=self.device)\n",
        "        optimizer = torch.optim.Adam(full_model.parameters(), lr=1e-3)\n",
        "\n",
        "        trainer = Trainer(full_model, optimizer, loss_fn=nn.MSELoss(), device=self.device)\n",
        "        trainer.train(train_loader, val_loader, epochs=50, patience=5)\n",
        "\n",
        "        evaluator = Evaluator()\n",
        "        results = evaluator.evaluate(full_model, test_loader, device=self.device)\n",
        "\n",
        "        print(\"Results:\")\n",
        "        for trait, metrics in results.items():\n",
        "            print(f\"{trait}: {metrics}\")\n",
        "\n",
        "        # Save results\n",
        "        import os, json\n",
        "        os.makedirs('results', exist_ok=True)\n",
        "        serializable_results = make_json_serializable(results)\n",
        "\n",
        "        filename = f\"results/{result_prefix}results_{task_type}_{fusion_type}.json\"\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(serializable_results, f, indent=4)\n",
        "\n",
        "        print(f\"✅ Results saved to {filename}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def get_input_dims(self, dataset):\n",
        "        sample, _ = dataset[0]\n",
        "        return {k: v.shape[-1] for k, v in sample.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afb6aea0-54f8-4d38-9a0a-e125d3086a30",
      "metadata": {
        "id": "afb6aea0-54f8-4d38-9a0a-e125d3086a30",
        "outputId": "1b89d29e-77dc-4a23-a822-11657d5eb07b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'reconstructed_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m     test_data = pickle.load(f)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Create runner\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m runner = ExperimentRunner(\u001b[43mreconstructed_train\u001b[49m, reconstructed_val, test_data)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Run CVAE-based middle tasks\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mmiddle_audio\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmiddle_vision\u001b[39m\u001b[33m'\u001b[39m]:\n",
            "\u001b[31mNameError\u001b[39m: name 'reconstructed_train' is not defined"
          ]
        }
      ],
      "source": [
        "# Load the updated CVAE pickles\n",
        "with open('reconstructed_cvae_train.pkl', 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "with open('reconstructed_cvae_val.pkl', 'rb') as f:\n",
        "    val_data = pickle.load(f)\n",
        "with open('cvae_test.pkl', 'rb') as f:  # Assuming this is already set up\n",
        "    test_data = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b9d8b08-0527-445f-92fd-f7321821e420",
      "metadata": {
        "id": "6b9d8b08-0527-445f-92fd-f7321821e420",
        "outputId": "a35fe4c4-902b-4722-f160-6085be403909"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['original_text', 'original_vision', 'original_audio', 'agreeableness', 'openness', 'neuroticism', 'extraversion', 'conscientiousness', 'reconstructed_visual', 'reconstructed_audio'])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[list(train_data.keys())[0]].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc197fe9-ffc5-4ff8-bdb1-93f968a1fa79",
      "metadata": {
        "scrolled": true,
        "id": "cc197fe9-ffc5-4ff8-bdb1-93f968a1fa79",
        "outputId": "efe2c1d8-9852-4066-da5c-0c28967a87ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['original_text', 'original_vision', 'original_audio', 'agreeableness', 'openness', 'neuroticism', 'extraversion', 'conscientiousness', 'reconstructed_visual', 'reconstructed_audio'])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_data[list(val_data.keys())[0]].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc30d59f-62dc-416a-9398-08bc3aebb548",
      "metadata": {
        "id": "dc30d59f-62dc-416a-9398-08bc3aebb548",
        "outputId": "6e2617d0-a345-45af-d304-533b76dbab5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['original_text', 'original_vision', 'original_audio', 'agreeableness', 'openness', 'neuroticism', 'extraversion', 'conscientiousness', 'generated_vision', 'generated_audio'])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data[list(test_data.keys())[0]].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "7ccb4d01-54c9-4433-ad1b-ed7e7d6a8d35",
      "metadata": {
        "id": "7ccb4d01-54c9-4433-ad1b-ed7e7d6a8d35"
      },
      "outputs": [],
      "source": [
        "# Create runner\n",
        "runner = ExperimentRunner(train_data, val_data, test_data)\n",
        "\n",
        "# Run CVAE-based middle tasks\n",
        "for task in ['middle_audio', 'middle_vision']:\n",
        "    for fusion in ['early', 'late']:\n",
        "        runner.run(task, fusion, result_prefix='cvae_')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38091c2e-cece-4b3a-95ef-a9531d443d50",
      "metadata": {
        "id": "38091c2e-cece-4b3a-95ef-a9531d443d50",
        "outputId": "034077d8-b15c-42d2-9b12-fa7e6787c1f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "comparison_table.csv\t\t       results_lower_audio_late.json\n",
            "cvae_results_middle_audio_early.json   results_lower_vision_early.json\n",
            "cvae_results_middle_audio_late.json    results_lower_vision_late.json\n",
            "cvae_results_middle_vision_early.json  results_middle_audio_early.json\n",
            "cvae_results_middle_vision_late.json   results_middle_audio_late.json\n",
            "diff_results_middle_audio_early.json   results_middle_vision_early.json\n",
            "diff_results_middle_audio_late.json    results_middle_vision_late.json\n",
            "diff_results_middle_vision_early.json  results_upper_early.json\n",
            "diff_results_middle_vision_late.json   results_upper_late.json\n",
            "pivot_ccc.csv\t\t\t       unified_comparison_table.csv\n",
            "pivot_mae.csv\t\t\t       unified_comparison_with_diffusion.csv\n",
            "results_lower_audio_early.json\n"
          ]
        }
      ],
      "source": [
        "!ls ./results/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a26f8dff-c8a8-4cd6-ac76-dbd13fe464bc",
      "metadata": {
        "id": "a26f8dff-c8a8-4cd6-ac76-dbd13fe464bc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Torch Env",
      "language": "python",
      "name": "torch-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}